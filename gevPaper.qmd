---
title: Differentiating Generalized Eigenvalues and Eigenvectors 
author: 
    - name: Jan de Leeuw
      orcid: 0000-0003-1420-1797
      email: jan@deleeuwpdx.net
      affiliation: 
        - name: University of California Los Angeles
          city: Los Angeles
          state: CA
          url: www.ucla.edu
      license: "CC0"
date: last-modified
date-format: long
bibliography: [mypubs.bib, total.bib]
number-sections: true
pdf-engine: lualatex
keep-tex: true
format:
   pdf:
    fontsize: 12pt
    include-in-header: 
     - preamble.tex
    keep-tex: true
    link-citations: true
    documentclass: scrartcl
    number-sections: true
   html:
    fontsize: 12pt
    include-in-header: 
     - preamble.css
    keep-md: true
    number-sections: true
toc: true
toc-depth: 3
editor: source
papersize: letter
graphics: true
link-citations: true
mainfont: Times New Roman
abstract: We give formulae for first and second derivatives of generalized eigenvalues/eigenvectors of symmetric matrices and generalized singular values/singular vectors of rectangular matrices when the matrices are linear or nonlinear functions of a vector of parameters. In addition, implementations in R (@r_core_team_25) are provided. Formulae are checked against Jacobians and Hessians computed by numerical differentiation. Some applications to multivariate data analysis are discussed.\sectionbreak
---

```{r loadpackages, echo = FALSE}
suppressPackageStartupMessages(library(knitr, quietly = TRUE))
suppressPackageStartupMessages(library(tinytex, quietly = TRUE))
suppressPackageStartupMessages(library(numDeriv, quietly = TRUE))
```

```{r code, echo = FALSE}
matrixPrint <- function(x,
                    digits = 6,
                    width = 8,
                    format = "f",
                    flag = "+") {
  print(noquote(formatC(
    x,
    digits = digits,
    width = width,
    format = format,
    flag = flag
  )))
}
```

**Note:** This is a working manuscript which will be expanded/updated frequently. All suggestions for improvement are welcome. All Rmd, tex, html, pdf, R, and C files are in the public domain. Attribution will be appreciated, but is not required. The files can be found at <https://github.com/deleeuw/generalized-eigen-derivatives>.

\sectionbreak

# Introduction

The generalized eigenvalue (GEV) problem for a pair $(A,B)$ of square symmetric matrices of order $n$ is to find a matrix $X$ of eigenvectors and a diagonal matrix $\Lambda$ of eigenvalues such that 
\begin{subequations}
\begin{align}
AX&=BX\Lambda,\label{eq-gevdef1}\\
X'BX&=I\label{eq-gevdef2}.
\end{align} 
\end{subequations} 
We assume that $B$ is positive definite and that the eigenvalues are in decreasing\footnote{By "decreasing" we mean $\lambda_1\geq\lambda_2\geq\cdots\geq\lambda_n$.} order along the diagonal.

The equations \eqref{eq-gevdef1} and \eqref{eq-gevdef2} implicitly define $(X(),\Lambda())$ as a function of $(A,B)$. It turns out that under suitable assumptions on the matrix arguments these implicit functions are actually differentiable, and this makes it interesting to compute their derivatives.

There is a humongous and scattered literature on formulae for and computations of derivatives of eigenvalues and eigenvectors in linear algebra, numerical mathematics, engineering, multivariate statistics, and even in physics. Reviewing and collecting all relevant literature is an impossible task. We only give the basic mathematical references, which provide the foundations upon which our results will be built (@kato_84, @baumgartel_85).

GEV is at the basis of almost all of the computations in multivariate statistics. In multinormal small sample theory the derivatives are needed for the Jacobians in change-of-variable calculations to find the distribution of many statistics. The applications we have in mind, however, are in large-sample statistics, where the derivatives are needed in Delta Method computations of standard errors and confidence intervals.

Again, the Delta Method literature is huge and diverse. Special cases have been around before there was a statistics discipline, ever since the beginning of error analysis in geodesy, physics, and astronomy (@gorroochurn_20). For the types of applications we have in mind all the relevant Delta Method details are given in @mann_wald_43 and @hsu_49.

We have written code in R (@r_core_team_25) to implement our final formulae for the first and second derivatives. Code is available at <https://github.com/deleeuw/generalized-eigen-derivatives>. In addition we have also
written R code, available in the same repository, to numerically verify the final formulae. This
uses the functions jacobian() and hessian() from the excellent R package numDeriv (@gilbert_varadhan_19).
Of course numerical verification, especially in the nonlinear case, is limited to a small number
of examples, but if we get the same results as from our formulae we do gain some confidence.

I should perhaps also mention a previous version of this paper (@deleeuw_R_07c), which has some errors and is not as complete as the current version.

\sectionbreak

# Basic Results

Suppose $A()$ and $B()$ are differentiable symmetric matrix valued functions of order $n$ on an open subset $\Theta$ of $\mathbb{R}^p$, and suppose at $\theta\in\Theta$ the matrix $B(\theta)$ is positive definite and the generalized eigenvalues of $(A(\theta),B(\theta))$ are all different. Then the ordered eigenvalues $\Lambda()$ and the eigenvectors $X()$ are analytic functions of the parameters in a neighborhood of $\theta$. 

We use subscripts $i,j=1,\cdots, n$ for the rows and columns of $A$ and $B$, subscripts $\eta,\nu=1,\cdots,n$ for the eigenvalues and eigenvectors, and $s,t=1,\cdots,p$ for the parameters in $\theta$. Thus $x_{i\nu}$ is element $i$ of eigenvector $x_\nu$. The partial derivative of a function $A()$ on 
$\Theta$ with respect to $\theta_s$, evaluated at $\theta$, is written as $\mathcal{D}_sA(\theta)$. In
order not to clutter our formulae the parameter vector where the derivative is evaluated is usually not
explicitly specified. Note also that it is sometimes necessary to use parentheses to distinguish $(\mathcal{D}_s)Ax$ from $\mathcal{D}_s(Ax)$.

## First Partials

Differentiate \eqref{eq-gevdef1} with respect to $\theta_s$. Then 
\begin{equation}
(\mathcal{D}_sA)X+A(\mathcal{D}_sX)=BX(\mathcal{D}_s\Lambda)+
B(\mathcal{D}_sX)\Lambda+(\mathcal{D}_sB)X\Lambda.\label{eq-firstder}
\end{equation} 
Premultiplying \eqref{eq-firstder} by $X'$ and rearranging gives 
\begin{equation}
\mathcal{D}_s\Lambda=\{X'(\mathcal{D}_sA)X-X'(\mathcal{D}_sB)X\Lambda\}+\{\Lambda X'B(\mathcal{D}_sX)-X'B(\mathcal{D}_sX)\Lambda\}.\label{eq-labfull}
\end{equation} 
The matrix $\Lambda X'B(\mathcal{D}_sX)-X'B(\mathcal{D}_sX)\Lambda$ is anti-symmetric and consequently has a zero diagonal. Taking the diagonal\footnote{The diagonal $\text{diag}(X)$ of a square matrix $X$ is a diagonal matrix with the same diagonal as $X$.} on both sides of \eqref{eq-labfull} gives 
\begin{equation}
\mathcal{D}_s\Lambda=\text{diag}\{X'(\mathcal{D}_sA)X-X'(\mathcal{D}_sB)X\Lambda\},\label{eq-labsol}
\end{equation} 
or, for a single eigenvalue $\lambda_i$ with corresponding eigenvector $x_i$, 
\begin{equation}
\mathcal{D}_s\lambda_\nu=x_\nu'(\mathcal{D}_sA-\lambda_\nu\mathcal{D}_sB)x_\nu.\label{eq-singval}
\end{equation}

Taking the off-diagonal\footnote{The off-diagonal $\text{off}(X)$ of a square matrix $X$ is $X$ with its diagonal replaced by zeroes.} on both sides of \eqref{eq-labfull} gives 
\begin{equation}
\text{off}\{\Lambda X'B(\mathcal{D}_sX)-X'B(\mathcal{D}_sX)\Lambda\}=-\text{off}\{X'(\mathcal{D}_sA)X-X'(\mathcal{D}_sB)X\Lambda\}.\label{eq-offd}
\end{equation} 
$X$ is non-singular, and thus there is a unique square $H_s$ such that $\mathcal{D}_sX=XH_s$. Using this substitution \eqref{eq-offd} becomes 
\begin{equation}
\text{off}\{\Lambda H_s-H_s\Lambda\}=-\text{off}\{X'(\mathcal{D}_sA)X-X'(\mathcal{D}_sB)X\Lambda\}.
\end{equation} 
Switch to subscript notation and solve for $H_s$. For $s\neq t$ 
\begin{equation}
\{H_s\}_{ij}=-\frac{x_i'\{\mathcal{D}_sA-\lambda_s\mathcal{D}_sB\}x_j}{\lambda_i-\lambda_j}.\label{eq-offh}
\end{equation} 
This does not give a value for the diagonal of $H$. Differentiating \eqref{eq-gevdef2} gives 
\begin{equation}
X'B(\mathcal{D}_sX)+(\mathcal{D}_sX)'BX+X'\mathcal{D}_sBX=0.\label{eq-fromtwo}
\end{equation} 
Using $\mathcal{D}_sX=XH_s$ and taking the diagonal of \eqref{eq-fromtwo} gives 
\begin{equation}
h_{ii}=-\tfrac12 x_i'(\mathcal{D}_sB)x_i. \label{eq-diagh}
\end{equation} 
Combining \eqref{eq-offh} and \eqref{eq-diagh} shows that for the eigenvector corresponding with $\lambda_s$ we have\footnote{Symbol $\cdot$ is used for multiplication with a scalar if using it makes a formula more clear, similar to parentheses.} then 
\begin{equation}
\mathcal{D}_sx_i=-\sum_{j\neq i}\frac{x_j'\{\mathcal{D}_sA-\lambda_i\mathcal{D}_sB\}x_i}{\lambda_j-\lambda_i}x_j-\tfrac12 x_i'(\mathcal{D}_sB)x_i\cdot x_i.\label{eq-secder1}
\end{equation}
which can also be written as
\begin{equation}
\mathcal{D}_sx_i=-\sum_{j\neq i}\left\{\frac{x_jx_j'}{\lambda_j-\lambda_i}\right\}(\mathcal{D}_sA-\lambda_i\mathcal{D}_sB)x_i-\tfrac12 x_i'(\mathcal{D}_sB)x_i)\cdot x_i.\label{eq-secder2}
\end{equation}
It is convenient to have a matrix expression for this derivative. Define, following @deleeuw_R_07c, the matrices
\begin{equation}
(A-\lambda_i B)^-:=
\sum_{j\neq i}\left\{\frac{x_jx_j'}{\lambda_j-\lambda_i}\right\}=
X(\Lambda-\lambda_iI)^+X',\label{eq-geninv}
\end{equation} 
where $(\Lambda-\lambda_iI)^+$ is the Moore-Penrose inverse of $\Lambda-\lambda_iI$. Matrix \eqref{eq-geninv}
is a weak inverse of $A-\lambda_i B$, in the sense that
\begin{equation}
(A-\lambda_i B)^-(A-\lambda_i B)(A-\lambda_i B)^-=(A-\lambda_i B)^-.\label{eq-2inverse}
\end{equation}
It is a Moore-Penrose inverse if and only if $B$ is identically equal to one (and thus $X'X=XX'=I$).

Using \eqref{eq-geninv} gives
\begin{equation}
\mathcal{D}_sx_i=-(A-\lambda_i B)^-(\mathcal{D}_sA-\lambda_i\mathcal{D}_sB)x_i-\tfrac12 x_i'(\mathcal{D}_sB)x_i\cdot x_i.\label{eq-singvec}
\end{equation}
Equations \eqref{eq-singval} and \eqref{eq-singvec} will be used frequently in this paper.

## Second Partials

To find second partial derivatives we start simple with the eigenvalues. Differentiate \eqref{eq-singval} with respect to $\theta_t$. This gives 
\begin{equation}
\mathcal{D}_{st}\lambda_i=2x_i'(\mathcal{D}_sA-\lambda_i\mathcal{D}_sB)\mathcal{D}_tx_i+
x_i'(D_{st}A-\lambda_s\mathcal{D}_{st}B)x_i-x_i'(\mathcal{D}_sB)x_i\cdot\mathcal{D}_t\lambda_i.\label{eq-seclbd}
\end{equation} 
Substituting from \eqref{eq-singval} and \eqref{eq-singvec} gives
\begin{multline}
\mathcal{D}_{st}\lambda_i=-2x_i'(\mathcal{D}_sA-\lambda_i\mathcal{D}_sB)W_i (\mathcal{D}_tA-\lambda_i\mathcal{D}_tB)x_i+
x_i'(D_{st}A-\lambda_i\mathcal{D}_{st}B)x_i\\-x_i'(\mathcal{D}_tB)x_i\cdot x_i'(\mathcal{D}_sA-\lambda_i\mathcal{D}_sB)x_i-x_i'(\mathcal{D_s}B)x_i\cdot x_i'(\mathcal{D}_tA-\lambda_i\mathcal{D}_tB)x_i.\label{eq-seclbdmat}
\end{multline} 
Formula \eqref{eq-seclbdmat} shows that $\mathcal{D}_{st}\lambda_i=\mathcal{D}_{ts}\lambda_i$, just as it should be.

The second partials of the generalized eigenvalues are, not surprisingly, more complicated. 
We start with \eqref{eq-secder1} and differentiate with respect to $\theta_t$. We give some
intermediate calculations for this case, because they make it easier to check the final
result. First
\begin{multline}
\mathcal{D}_{st}x_i=-\sum_{j\neq i}\mathcal{D}_t\left\{\frac{x_j'\{\mathcal{D}_sA-\lambda_i\mathcal{D}_sB\}x_i}{\lambda_j-\lambda_i}\right\}x_j\\
-\sum_{j\neq i}\frac{x_j'\{\mathcal{D}_sA-\lambda_i\mathcal{D}_sB\}x_i}{\lambda_j-\lambda_i}\mathcal{D}_tx_j
-\tfrac12\mathcal{D}_t\{x_i'(\mathcal{D}_sB)x_i\cdot x_i\}.\label{eq-inter1}
\end{multline}
The terms in the first summation in \eqref{eq-inter1} are
\begin{multline}
\mathcal{D}_t\left\{\frac{x_j'\{\mathcal{D}_sA-\lambda_i\mathcal{D}_sB\}x_i}{\lambda_j-\lambda_i}\right\}=\\
\frac{\mathcal{D}_t\{x_j'(\mathcal{D}_sA-\lambda_i\mathcal{D}_sB)x_i\}}{\lambda_i-\lambda_j}
-\frac{(\mathcal{D}_t\lambda_j-\mathcal{D}_t\lambda_i)x_j'(\mathcal{D}_sA-\lambda_i\mathcal{D}_sB)x_i}{(\lambda_j-\lambda_i)^2},\label{eq-inter2}
\end{multline}
and the numerator in the first term of \eqref{eq-inter2} is
\begin{multline}
\mathcal{D}_t\{x_j'(\mathcal{D}_sA-\lambda_i\mathcal{D}_sB)x_i\}=(\mathcal{D}_tx_j)'(\mathcal{D}_sA-\lambda_i\mathcal{D}_sB)x_i+x_j'(\mathcal{D}_sA-\lambda_i\mathcal{D}_sB)\mathcal{D}_tx_i\\+
x_j'(\mathcal{D}_{st}A-\lambda_i\mathcal{D}_{st}B)x_i-\mathcal{D}_t\lambda_i\cdot x_j'(\mathcal{D}_sB)x_i.
\label{eq-inter3}
\end{multline}
Finally
\begin{multline}
\mathcal{D}_t\{x_i'(\mathcal{D}_sB)x_i\cdot x_i\}=\\2(\mathcal{D}_tx_i)'(\mathcal{D}_sB)x_i\cdot x_i+x_i'(\mathcal{D}_{st}B)x_i\cdot x_i+x_i'(\mathcal{D}_tB)x_i\cdot\mathcal{D}_tx_i.\label{eq-inter4}
\end{multline}
In computing the final result we first evaluate \eqref{eq-inter3}, substitute the result in \eqref{eq-inter2},
and then substitute that result, together with the result of \eqref{eq-inter4}, into \eqref{eq-inter1}.
For this to work we need to have both $\mathcal{D}\Lambda$ and $\mathcal{D}X$ available. We could of course
substitute the expressions of $\mathcal{D}\Lambda$ and $\mathcal{D}X$ to \eqref{eq-inter1}-\eqref{eq-inter4}
but that would result in a very long and opaque formula. We will rely on our software to give us the
numerical values for any specific value of $\theta$.

\sectionbreak

# Software

In the [github repository](https://www.github.com/deleeuw/generalized-eigen-derivatives) there are R programs
evaluating $A, B, \mathcal{D}\Lambda, \mathcal{D}X, \mathcal{D}^2\Lambda,$ and $\mathcal{D}^2X$
at a given $\theta$. The function gevNonlinear() can be used for any non-linear model. Its only argument
is the vector $\theta$. It computes the first partials and then calls the subroutines gevHessianValues()
and gevHessianVectors() to compute the second partials. The gevHessianVectors() routine uses the
stepwise method suggested at the end of the previous section. First compute all first derivatives, and then
use their numerical values in equations \eqref{eq-inter1}-\eqref{eq-inter4}.

In order to attain the necessary level of generality gevNonlinear() needs to be run from or after a template which defines global variables, specifically the R functions theA(), theB(), dA(), dB(), ddA(), and ddB(), together with the parameters and additional values that these auxiliary functions need. 

The result is a list with

* $A(\theta)$, an $n\times n$ matrix, typical element $a_{ij}(\theta)$,
* $B(\theta)$, an $n\times n$ matrix, typical element $b_{ij}(\theta)$,
* $\lambda(\theta)$, an $n$-element vector, typical element $\lambda_\nu(\theta)$,
* $X(\theta)$, an $n\times n$ matrix, typical element $x_{i\nu}(\theta)$,
* $\mathcal{D}\Lambda(\theta)$, an $n\times p$ matrix, element $(\nu, s)$ is $\mathcal{D}_s\lambda_\nu(\theta)$,
* $\mathcal{D}X(\theta)$, an $n\times p\times n$ array, element $(i, s, \nu)$ is $\mathcal{D}_sx_{i\nu}(\theta)$,
* $\mathcal{D}^2\Lambda(\theta)$, a $p\times p\times n$ array,element $(s, t, \nu)$
is $\mathcal{D}_{st}\lambda_\nu(\theta)$,
* $\mathcal{D}^2X(\theta)$, a $p\times p\times n\times n$ array, element 
$(s, t, i, \nu)$ is $\mathcal{D}_{st}x_{i\nu}(\theta)$.

The names in the code and in the results for the eight matrices in this list are a, b, l, x, dl, dx, ddl, and ddx. The print function prints $\mathcal{D}\Lambda$ as an $n\times p$ matrix and $\mathcal{D}X$ as a sequence of $n$ matrices of dimensions $n\times p$, one for each eigenvalue. $\mathcal{D}^2\Lambda$ is printed
as $n$ symmetric matrices of order $p$, and $\mathcal{D}^2X$ as $\smash{n^2}$ symmetric matrices of order $p$, one for each element of $X$.

There is a corresponding numDeriv based check function gevNonlinearNum(), which has the same single argument $\theta$
and gives the same output as gevNonlinear(). In addition there is gevMyLimit(), which is a quick and dirty
numerical differentiation routine based on
\begin{subequations}
\begin{equation}
\mathcal{D}f_s(\theta)=\lim_{\epsilon\rightarrow 0}\frac{f(\theta+\epsilon e_s)-f(\theta)}{\epsilon},\label{eq-lim1}
\end{equation}
and
\begin{equation}
\mathcal{D}f_{st}(\theta)=\lim_{\epsilon\rightarrow 0}\frac{f(\theta+\epsilon(e_s+e_t))-f(\theta+\epsilon e_s)-f(\theta+\epsilon e_t)+f(\theta)}{\epsilon^2}.\label{eq-lim2}
\end{equation}
\end{subequations}
The function gevMyLimit() uses a single small value of $\epsilon$, by default $10^{-5}$. The function can be applied to any eigenvalue or eigenvector element and in a loop can produce any of the matrices of first or second partials. By default $\epsilon$
is $10^{-5}$. If $\epsilon$ is too small in gevMyLimit() the approximation is overwhelmed by the rounding error that comes from squaring $\epsilon$ in the denominator of \eqref{eq-lim2}.

\sectionbreak

# Examples

This section gives two examples of templates that uses artificial random data. We do not give the numerical results of the various
derivative calculations, which for these examples are just a large amount of meaningless numbers. The software
in the repository, together with the templates, can be used to calculate Jacobians and Hessians for various
values of the parameter vector. I should perhaps mention that in both examples the results for the derivatives based on the formulas are the same as the results based on numerical differentiation.

## Single Parameter

In our first example we define
\begin{align}
A(\theta)&=A_0+\theta A_1+\frac12\theta^2A_2+...+\frac{1}{p}\theta^pA_p\\
B(\theta)&=B_0+\theta B_1+\frac12\theta^2B_2+...+\frac{1}{p}\theta^pB_p
\end{align}
The template for a special case with $n=4$ and $p=3$ is gevExampleOne.R in 
the [github repository](https://www.github.com/deleeuw/generalized-eigen-derivatives).

## Linear Combinations

Suppose $A()$ is linear combinations of $p$ known symmetric matrices $A_s$ and $B()$ is a linear combination of
$p$ known symmetric matrices $B_s$. There may also be "intercepts" $A_0$ and $B_0$. Thus
\begin{subequations}
\begin{align}
A(\theta)&=A_0+\sum_{s=1}^p\theta_sA_s,\\
B(\theta)&=B_0+\sum_{s=1}^p\theta_sB_s.
\end{align}
\end{subequations}
It may seem somewhat limiting that the same parameter vector $\theta$ is used for both $A()$ and $B()$. But
we can decide to make the last $q$ matrices $A_s$ and the first $p-q$ matrices $B_s$ equal to zero. This means, in effect, that $A()$ and $B()$ depend on different parameter vectors.

The template for a special case with $n=4$ and $p=6$ is gevExampleTwo.R in 
the [github repository](https://www.github.com/deleeuw/generalized-eigen-derivatives).
In this example the last three $A_s$ and the first three $B_s$ are indeed zero.


\sectionbreak

# Special Results

The function gevNonlinear() can handle any parametric model, as long as the appropriate template is provided with routines for $A, B, \mathcal{D}A, \mathcal{D}B, \mathcal{D}^2A,$ and $\mathcal{D}^2B$.
Nevertheless in special cases more compact and elegant formulae and more efficient computations
will undoubtedly exist. The question then becomes if it is worthwhile to retype the formulae and to 
reprogram the R routines for these special cases.

## Linear Combinations

The example in which $A()$ and $B()$ are linear combinations of known matrices is important enough
to have its own batch of formulae for the derivatives. The basic simplication is of course that $\mathcal{D}_sA=A_s$ and $\mathcal{D}_sB=B_s$, and that consequently the second partials $\mathcal{D}_{st}A$ and $\mathcal{D}_{st}B$ are all zero.

For the first derivatives we find
\begin{align}
\mathcal{D}_s\lambda_i&=x_i'(A_s-\lambda_iB_s)x_i,\label{eq-singvalcon}\\
\mathcal{D}_sx_i&=-(A-\lambda_i B)^-(A_s-\lambda_iB_s)x_i-\tfrac12 x_i'B_sx_i\cdot x_i.\label{eq-singveccon}
\end{align}
The second partials are
\begin{multline}
\mathcal{D}_{st}\lambda_i=-2x_i'(A_s-\lambda_iB_s)(A-\lambda_i B)^-(A_t-\lambda_iB_t)x_i\\-x_i'B_tx_i\cdot x_i'(A_s-\lambda_iB_s)x_i-x_i'B_sx_i\cdot x_i'(A_t-\lambda_iB_t)x_i,\label{eq-seclbdmatcon}
\end{multline} 
and
\begin{align}
\mathcal{D}_{st}x_i=&-\mathcal{D}_t(A-\lambda_i B)^-(A_s-\lambda_iB_s)x_i\notag\\
&+\mathcal{D}_t\lambda_i\cdot (A-\lambda_i B)^-B_sx_i\notag\\
&-(A-\lambda_i B)^-(A_s-\lambda_iB_s)\mathcal{D}_tx_i\notag\\
&-(\mathcal{D}_tx_i)'B_sx_i\cdot x_i\notag\\
&-\tfrac12 x_i'B_sx_i\cdot\mathcal{D}_tx_i.\label{eq-hessgen}
\end{align}

## Elementwise Partial Derivatives

As a special case of the linear combinations we have the partial derivatives with respect to the elements 
of $A$ and $B$. 
\begin{subequations}
\begin{align}
A&=\mathop{\sum\sum}_{1\leq i\leq j}a_{ij}E_{ij},\\
B&=\mathop{\sum\sum}_{1\leq i\leq j}b_{ij}E_{ij}.
\end{align}
\end{subequations} 
Here $E_{ij}:=(e_ie_j'+e_je_i')$ for $i\neq j$ and $E_{ii}=e_ie_i'$ with $e_i$ and $e_j$ unit vectors\footnote{A unit vector $e_i$ has zeroes everywhere, except for element $i$, which is one.}. Thus \begin{equation}
\mathcal{D}_{(i,j)}A=\mathcal{D}_{(i,j)}B=E_{ij}.
\end{equation} 
and, with some improvised but obvious notation, 
\begin{subequations}
\begin{align}
\mathcal{D}_{(i,j)}^A\lambda_s&=
\begin{cases}
2x_{is}x_{js}&\text{ if }i\neq j,\\
x_{is}^2&\text{ if }i = j.
\end{cases}\\
\mathcal{D}_{(i,j)}^B\lambda_s&=
\begin{cases}
-2\lambda_sx_{is}x_{js}&\text{ if }i\neq j\\,
-\lambda_sx_{is}^2&\text{ if }i = j.
\end{cases}
\end{align}
\end{subequations}

## Simple Eigenvalue Decomposition

A simple eigenvalue (SEV) problem is of the form 
\begin{subequations}
\begin{align}
AX&=X\Lambda,\\
X'X&=I.
\end{align} 
\end{subequations}
Thus $B=I$ and $\mathcal{D}B=0$. In SEV the matrix $(A-\lambda_iB)^-$ from \eqref{eq-geninv} is actually the Moore-Penrose inverse of $A-\lambda_iI$.

From \eqref{eq-singval} 
\begin{equation}
\mathcal{D}_s\lambda_i=x_i'(\mathcal{D}_sA)x_i.\label{eq-sevalsim}
\end{equation} 
and from 
\eqref{eq-singvec} 
\begin{equation}
\mathcal{D}_sx_i=-(A-\lambda_iI)^+(\mathcal{D}_sA)x_i.\label{eq-vecpersimmp}
\end{equation}

$$
\mathcal{D}_{st}\lambda_i=-2x_i'(\mathcal{D}_sA)(A-\lambda_iI)^+(\mathcal{D}_tA)x_i+
x_i'(D_{st}A)x_i.
$$

\begin{align}
\mathcal{D}_{st}x_i=&-\mathcal{D}_t(A-\lambda_i I)^+(\mathcal{D}_sA)x_i\notag\\
&-(A-\lambda_i I)^+(\mathcal{D}_{st}A)x_i\notag\\
&-(A-\lambda_i I)^+(\mathcal{D}_{s}A)\mathcal{D}_tx_i.\label{eq-hessgensimple}
\end{align}

## Generalized Singular Value Decomposition

\begin{subequations}
\begin{align}
FY&=GX\Lambda,\label{eq-gsvdef1a}\\
F'X&=HY\Lambda,\label{eq-gsvdef2a}\\
X'GX&=I,\label{eq-gsvdef3a}\\
Y'HY&=I,\label{eq-gsvdef4a}
\end{align}
\end{subequations}

All three matrices $F$, $G$, and $H$
are assumed to be functions of the parameters $\theta$. Instead of tackling the GSV problem directly, we
reduce it to an equivalent GEV problem.

From \eqref{eq-gsvdef1a} $X=G^{-1}FY\Lambda^{-1}$ and consequently $Y$ and $\Lambda$ 
solve the GEV problem.
\begin{equation}
F'G^{-1}FY=HY\Lambda^2.
\end{equation}
Now if $A=F'G^{-1}F$ and $B=H$ we have 
$$
\mathcal{D}_sA=\mathcal{D}_s(F'G^{-1}F)=(\mathcal{D}_sF)'G^{-1}F+F'G^{-1}(\mathcal{D}_sF)-F'G^{-1}(\mathcal{D}_sG)G^{-1}F
$$ 
and $\mathcal{D}_sB=\mathcal{D}_sH$. Consequently from \eqref{eq-singval}
$$
\mathcal{D}_s\lambda_i^2=y_i'\{(\mathcal{D}_sF)'G^{-1}F+F'G^{-1}(\mathcal{D}_sF)-F'G^{-1}(\mathcal{D}_sG)G^{-1}F-\lambda_i^2\mathcal{D}_sH\}y_i
$$
This can be simplified by using $G^{-1}Fy_i=\lambda_ix_i$ and $\mathcal{D}_s\lambda_i^2=2\lambda_i\mathcal{D}_s\lambda_i$, so that
$$
\mathcal{D}_s\lambda_i=x_i'(\mathcal{D}_sF)y_i-\frac12\lambda_i\{x_i'(\mathcal{D}_sG)x_i+y_i'(\mathcal{D}_sH)y_i\}
$$

$$
\mathcal{D}_sy_i=-\sum_{j\neq i}\frac{\lambda_i^{\ } x_i'(\mathcal{D}_sF)y_j+\lambda_j^{\ }x_j'(\mathcal{D}_sF)y_i-\lambda_i\lambda_jx_i'(\mathcal{D}_sG)x_j-\lambda_i^2y_i'(\mathcal{D}_sH)y_j}{\lambda_j^2-\lambda_i^2}y_j-\tfrac12 y_i'(\mathcal{D}_sH)y_i\cdot y_i.
$$ 

In the same way from \eqref{eq-gsvdef2a} $Y=H^{-1}F'X\Lambda^{-1}$, and thus
\begin{equation}
FH^{-1}F'X=GX\Lambda^2.
\end{equation}
For this GEV 
$$
\mathcal{D}_sA=\mathcal{D}_s(FH^{-1}F')=(\mathcal{D}_sF)H^{-1}F'+FH^{-1}(\mathcal{D}_sF)'-FH^{-1}(\mathcal{D}_sH)H^{-1}F'
$$
which implies
$$
\mathcal{D}_sx_i=-\sum_{j\neq i}\frac{\lambda_ix_j'(\mathcal{D}_sF)y_i+\lambda_jx_i'(\mathcal{D}_sF)y_j-\lambda_i\lambda_jy_i'(\mathcal{D}_sH)y_j-\lambda_i^2x_i'(\mathcal{D}_sG)x_j}{\lambda_j-\lambda_i}x_j-\tfrac12 x_i'(\mathcal{D}_sG)x_i\cdot x_i
$$

\sectionbreak

# Applications

## Multiple Correspondence Analysis

In Multiple Correspondence Analysis (MCA) we have $N$ observations
on $m$ categorical variables (also known as "factors"). Variable
$j$ has $k_j$ possible values (also known as "levels"), and our observations are coded
as unit vector of length $k_j$, concatenated to binary vectors (also known as "profiles")
of length $K:=\sum k_j$. There are $p:=\prod k_j$ possible observations,
and each of them occurs in the data with relative frequency $\pi_s$ (in most MCA applications
$N<<M$ and thus many $\pi_s$ will be zero). Note that (ordinary) Correspondence
Analysis (CA) is the special case with $m=2$.

The Burt matrix $A$ is of the form
$$
A(\pi)=\sum_{s=1}^p \pi_s g_s g_s'
$$
while $B$ is
$$
B(\pi)=m\sum_{s=1}^p \pi_s\ \text{diag}(g_s g_s').
$$
The first derivatives with respect to $\pi_s$ are given by \eqref{eq-singvalcon} and \eqref{eq-singveccon}, using $G_s:=m\text{diag}(g_sg_s')$. 
\begin{align}
\mathcal{D}_s\lambda_i&=(x_i'g_s)^2-\lambda_ix_i'G_sx_i,\label{eq-singvalconmca}\\
\mathcal{D}_sx_i&=-(A-\lambda_i B)^-(g_sg_s'-\lambda_iG_s)x_i-\tfrac12 x_i'G_sx_i\cdot x_i.\label{eq-singvecconmca}
\end{align}
It follows that
\begin{align}
\sum_{s=1}^m\pi_s\mathcal{D}_s\lambda_i&=0,\\
\sum_{s=1}^m\pi_s\mathcal{D}_sx_i&=-\frac12 x_i.
\end{align}
Also, from the Delta Method, the asymptotic covariance matrix under multinomial sampling is given by
$$
N\cdot \text{ACOV}(\lambda_i,\lambda_j)=\sum_{s=1}^p\pi_s(\mathcal{D}_s\lambda_i)(\mathcal{D}_s\lambda_j)
$$
$$
N\cdot \text{ACOV}(x_i,x_j)=\sum_{s=1}^p\pi_s(\mathcal{D}_sx_i)(\mathcal{D}_sx_j)'-\frac14 x_ix_j'
$$
MCA is a popular technique, and thus it deserves a "real" example. We use the three-dimensional 
contingency table HairEyeColor from the R datasets package, which is a part of base R (@r_core_team_25). 
```{r readdata, echo = FALSE}
library(datasets)
data("HairEyeColor")
HairEyeColor
```
The first variable is hair color (4 levels), the second is 2 is eye color (4 levels), and the third variable is sex (2 levels).
There are 592 cross-classified individuals, students in an introductory statistics course at the University
of Delaware. Thus $N=592$, $p=4\times 4\times 2=32$, $n=4+4+2=10$.

It is abundantly clear that this example shoulds be analyzed with some special purpose software
which does not require computing and storing the $M$ arrays $A_s$ and the $M$ arrays $B_s$, which are
symmetric, integer, and very sparse. In this paper we want to show, however, that gevNonlinear() can do 
it all. Thus we make a template gevExampleMCA.R, which can actually handle any three-dimensional table.
After running the template, we apply gevNonlinear().

```{r do comp, echo = FALSE, cache = TRUE}
source("gevExampleMCA.R")
h <- gevNonlinear(theta)
```
In MCA the dominant eigenvalue is always equal to one, and there are always $m-1$ eigenvalues equal to zero. These "trivial" dimensions are not a property of the data, but jusrt a consequence of the indicator coding.
There are only $p-m$ non-trivial dimensions, which for our example is seven. For the dominant trivial
dimension the derivatives are always zero, for the $m-1$ zero eigenvalues the derivatives are not
even defined. We only give the results for the non-trivial dimensions.

The eigenvalues are
```{r evalhec, echo = FALSE}
matrixPrint(h$l[2:8], digits = 3, width = 4)
```
and the eigenvectors are
```{r evechec, echo = FALSE}
matrixPrint(h$x[,2:8], digits = 3, width = 4)
```
The asymptotic covariance of the eigenvalues, multiplied by $N = 592$, is
```{r acoveval}
ac <- h$dl %*% diag(theta) %*% t(h$dl)
matrixPrint(ac[2:8, 2:8], digits = 4)
```
and thus the standard errors of the eigenvalues are
```{r}
matrixPrint(sqrt(diag(ac)[2:8] / 592), digits = 4)
```
There are seven non-trivial eigenvectors, and thus 21 non-trivial asymptotic covariance
matrices of order ten. We only print out the covariance matrix (times $N$) of the 
first non-trivial eigenvector.

```{r acovevec}
hh <- h$dx[, , 2]
ac <- hh %*% diag(theta) %*% t(hh)
ac <- ac - outer(h$x[, 2], h$x[, 2]) / 4
matrixPrint(ac, digits = 2, width = 3)
```
The standard errors of the elements of the dominant eigenvector are
```{r}
matrixPrint(sqrt(diag(ac) / 592), digits = 4)
```

In actual MCA applications the eigenvectors are usually scaled by multiplying them with the
square root of the corresponding eigenvalue. This makes the computation of the
asymptotic distribution of the scaled eigenvectors slightly more complicated.

The Delta Method
tells us second derivatives  can be used to give estimates of the bias, using
$$
N\cdot\text{BIAS}(\lambda_i)=\frac12\text{tr}\ \mathcal{D}^2\lambda_i(\Pi-\pi\pi')=\frac12\sum_{s=1}^p\pi_s\mathcal{D}_{ss}\lambda_i,
$$
because $\pi'(\mathcal{D}^2\lambda_i)\pi=0$.

```{r}
bias <- rep(0, 7)
for (r in 2:8) {
  dd <- h$ddl[, , r]
  bias[r - 1] <- sum(diag(dd) * theta) / (2 * 592)
}
matrixPrint(bias, digits = 4)
```
We see that the larger eigenvalues are overestimated and the smaller ones are underestimated, as expected.


## Covariances and Correlations

The results for MCA derived in the previous section can be applied directly
to the situation in which the possible data are selected from a finite number of numerical
vectors. Because of the limits of measurement precision one could argue that all
multivariate data are actually of this type. But of course for so-called
"continuous" data the number of possible values, although finite, can be
enormously large. One solution for the discrete-continuous dilemma is to 
move from expectations that are weighted sums to expectations that are
weighted by the empirical distribution function. We will go a different way.

Note that

$$
A(\theta)=\sum_{s=1}^N \theta_sg_sg_s'
$$
$$
\underline{\theta}_s=N^{-1}+N^{-\frac12}\underline{\omega}_s
$$
$$
\underline{\omega}_s:=N^\frac12(\underline{\theta}_s-N^{-1})
$$
(Infinitesimal) Jackknife

## Covariance Factor Analysis

MINRES

Swain

## Matrix Decomposition Factor Analysis

In Matrix Decomposition Factor Analysis (MDFA) we have to maximize the sum of the
largest singular values of a matrix $XT$ over $T$, which is equivalent to
maximizing the sum of the square roots of the largest eigenvalues of 
$T'X'XT$. The matrix $X$ is $n\times m$ and $T$ is $m\times p$, with $m<p$.
Thus $A=T'X'XT$ is of order $p$ and of rank $r\leq m$. We suppose it is
locally of constant rank $m$ and we maximize the sum of the sq1uare roots
of its $m$ non-zero eigenvalues. The matrix $T$ is patterned, which means
that some of its elments are restricted to be equal to zero.

The pattern is defined by a basis of known matrices $T_s$ with the same
pattern, so that
$$
T=\sum_{s=1}^p\theta_sT_s
$$
The problem is to maximize
$$
\sum_{j=1}^m\sqrt{\lambda_j(\theta_s\theta_tT_s'CT_t)}
$$
$$
\mathcal{D}_sA=T'CT_s+T_s'CT
$$
$$
\mathcal{D}_{st}A=T_s'CT_t+T_t'CT_s
$$
$$
\sum_{j=1}^m\mathcal{D}_s\lambda_j^\frac12=\frac12\sum_{j=1}^m\lambda_j^{-\frac12}\mathcal{D}_s\lambda_j=
\frac12\sum_{j=1}^m\lambda_j^{-\frac12}x_j'(\mathcal{D}_sA)x_j=\frac12\text{tr}\ A^{-\frac12}\mathcal{D}_sA=\text{tr}\ A^{-\frac12}T'CT_s
$$
$$
\sum_{j=1}^m\mathcal{D}_{st}\lambda_j^\frac12=-\frac14\sum_{j=1}^m\lambda_j^{-\frac32}(\mathcal{D}_t\lambda_j)(\mathcal{D}_s\lambda_j)+\frac12\sum_{j=1}^m\lambda_j^{-\frac12}\mathcal{D}_{st}\lambda_j
$$ 
\sectionbreak

# References
