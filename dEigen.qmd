---
title: Differentiating Generalized Eigenvalues/Vectors and Singular Values/Vectors 
author: 
    - name: Jan de Leeuw
      orcid: 0000-0003-1420-1797
      email: jan@deleeuwpdx.net
      affiliation: 
        - name: University of California Los Angeles
          city: Los Angeles
          state: CA
          url: www.ucla.edu
      license: "CC0"
date: last-modified
date-format: long
bibliography: [mypubs.bib, total.bib]
number-sections: true
pdf-engine: lualatex
keep-tex: true
format:
   pdf:
    fontsize: 12pt
    include-in-header: 
     - preamble.tex
    keep-tex: true
    link-citations: true
    documentclass: scrartcl
    number-sections: true
   html:
    fontsize: 12pt
    include-in-header: 
     - preamble.css
    keep-md: true
    number-sections: true
toc: true
toc-depth: 3
editor: source
papersize: letter
graphics: true
link-citations: true
mainfont: Times New Roman
abstract: We give formulas for first and second derivatives of generalized eigenvalues/eigenvectors and generalized singular values/singular vectors. In addition, implementations in R (@r_core_team_25) are provided. Formulas are checked against Jacobians and Hessians computed by numerical differentiation. Some applications to multivariate data analysis are discussed.\sectionbreak
---

```{r loadpackages, echo = FALSE}
suppressPackageStartupMessages(library(knitr, quietly = TRUE))
suppressPackageStartupMessages(library(tinytex, quietly = TRUE))
suppressPackageStartupMessages(library(numDeriv, quietly = TRUE))
```

```{r code, echo = FALSE}
source("dEigen.R")
matrixPrint <- function(x,
                    digits = 6,
                    width = 8,
                    format = "f",
                    flag = "+") {
  print(noquote(formatC(
    x,
    digits = digits,
    width = width,
    format = format,
    flag = flag
  )))
}
```

**Note:** This is a working manuscript which will be expanded/updated frequently. All suggestions for improvement are welcome. All Rmd, tex, html, pdf, R, and C files are in the public domain. Attribution will be appreciated, but is not required. The files can be found at <https://github.com/deleeuw/gevd>

\sectionbreak

# Introduction

The generalized eigenvalue (GEV) problem for a pair $(A,B)$ of square symmetric matrices of order $n$ is to find a matrix $X$ of eigenvectors and a diagonal matrix $\Lambda$ of eigenvalues such that 
\begin{subequations}
\begin{align}
AX&=BX\Lambda,\label{eq-gevdef1}\\
X'BX&=I\label{eq-gevdef2}.
\end{align} 
\end{subequations} 
We assume that $B$ is positive definite and that the eigenvalues are in decreasing\footnote{By "decreasing" we mean $\lambda_1\geq\lambda_2\geq\cdots\geq\lambda_n$.} order along the diagonal.

A generalized singular value (GSV) problem is defined by a triple of matrices $(F,G,H)$. Here $F$ is $n\times m$, with $n\geq m$, $G$ is $n\times n$, and $\H$ is $m\times m$. In GSV we solve the system \begin{subequations}
\begin{align}
FY&=GX\Lambda,\label{eq-gsvdef1}\\
F'X&=HY\Lambda,\label{eq-gsvdef2}\\
X'GX&=I,\label{eq-gsvdef3}\\
Y'HY&=I,\label{eq-gsvdef4}
\end{align}
\end{subequations} for the unknowns $X, Y,$ and $\Lambda$. The columns of $X$ are the left singular vectors, the columns of $Y$ the right singular vectors. Both $G$ and $H$ are assumed to be positive definite. The singular values in $\Lambda$ are non-negative, and in decreasing order along the diagonal.

The equations \eqref{eq-gevdef1} and \eqref{eq-gevdef2} implicitly define $(X(),\Lambda())$ as a function of $(A,B)$. In the same way \eqref{eq-gsvdef1} to \eqref{eq-gsvdef4} implicitly define $(X(),Y(),\Lambda())$ as a function of $(F,G,H)$. It turns out that under suitable assumptions on the matrix arguments these implicit functions are actually differentiable, and this makes it interesting to compute their derivatives.

There is a humongous and scattered literature on formulas for and computations of derivatives of eigenvalues and eigenvectors in linear algebra, numerical mathematics, engineering, multivariate statistics, and even in physics. Reviewing and collecting all relevant literature is an impossible task. We only give the basic mathematical references, which provide the foundations upon which our results will be built (@kato_84, @baumgartel_85).

GEV and GSV are at the basis of almost all of the computations in multivariate statistics. In multinormal small sample theory the derivatives are needed for the Jacobians in change-of-variable calculations to find the distribution of many statistics. The applications we have in mind, however, are in large-sample statistics, where the derivatives are needed in Delta Method computations of standard errors and confidence intervals.

Again, the Delta Method literature is huge and diverse. Special cases have been around before there was a statistics discipline, ever since the beginning of error analysis in geodesy, physics, and astronomy (@gorroochurn_20). For the types of applications we have in mind all the relevant Delta Method details are given in @mann_wald_43 and @hsu_49.

I should perhaps also mention a previous version of this paper (@deleeuw_R_07c), which has some errors and is not as complete as the current version.

\sectionbreak

# Basic Results

Suppose $A()$ and $B()$ are differentiable symmetric matrix valued functions of order $n$ on an open subset $\Theta$ of $\mathbb{R}^p$, and suppose at $\theta\in\Theta$ the matrix $B(\theta)$ is positive definite and the generalized eigenvalues of $(A(\theta),B(\theta))$ are all different. Then the ordered eigenvalues $\Lambda()$ and the eigenvectors $X()$ are differentiable functions of the parameters in a neighborhood of $\theta$. We use subscripts $i,j,k,l=1,\cdots, n$ for the elements of $A,B,\Lambda,$ and $X$, and $s,t=1,\cdots,p$ for the parameters in $\theta$. The partial derivative of a function $A$ on 
$\Theta$ with respect to $\theta_s$, evaluated at $\theta$, is $\mathcal{D}_sA(\theta)$. In
order not to clutter our formulas the place where the derivative is evaluated is usually not
explicitly specified. Note also that it is sometimes necessary to 
use parenthesis to distinguish $(\mathcal{D}_s)Ax$ from $\mathcal{D}_s(Ax)$.

## First Partials

Differentiate \eqref{eq-gevdef1} with respect to $\theta_s$. Then 
\begin{equation}
(\mathcal{D}_sA)X+A(\mathcal{D}_sX)=BX(\mathcal{D}_s\Lambda)+
B(\mathcal{D}_sX)\Lambda+(\mathcal{D}_sB)X\Lambda.\label{eq-firstder}
\end{equation} 
Premultiplying \eqref{eq-firstder} by $X'$ and rearranging gives 
\begin{equation}
\mathcal{D}_s\Lambda=\{X'(\mathcal{D}_sA)X-X'(\mathcal{D}_sB)X\Lambda\}+\{\Lambda X'B(\mathcal{D}_sX)-X'B(\mathcal{D}_sX)\Lambda\}.\label{eq-labfull}
\end{equation} 
The matrix $\Lambda X'B(\mathcal{D}_sX)-X'B(\mathcal{D}_sX)\Lambda$ is anti-symmetric and consequently has a zero diagonal. Taking the diagonal\footnote{The diagonal $\text{diag}(X)$ of a square matrix $X$ is a diagonal matrix with the same diagonal as $X$.} on both sides of \eqref{eq-labfull} gives 
\begin{equation}
\mathcal{D}_s\Lambda=\text{diag}\{X'(\mathcal{D}_sA)X-X'(\mathcal{D}_sB)X\Lambda\},\label{eq-labsol}
\end{equation} 
or, for a single eigenvalue $\lambda_i$ with correspondiung eigenvector $x_i$, 
\begin{equation}
\mathcal{D}_s\lambda_i=x_i'(\mathcal{D}_sA-\lambda_s\mathcal{D}_sB)x_i.\label{eq-singval}
\end{equation}

Taking the off-diagonal\footnote{The off-diagonal $\text{off}(X)$ of a square matrix $X$ is $X$ with its diagonal replaced by zeroes.} on both sides of \eqref{eq-labfull} gives 
\begin{equation}
\text{off}\{\Lambda X'B(\mathcal{D}_sX)-X'B(\mathcal{D}_sX)\Lambda\}=-\text{off}\{X'(\mathcal{D}_sA)X-X'(\mathcal{D}_sB)X\Lambda\}.\label{eq-offd}
\end{equation} 
$X$ is non-singular, and thus there is a unique square $H_s$ such that $\mathcal{D}_sX=XH_s$. Using this substitution \eqref{eq-offd} becomes 
\begin{equation}
\text{off}\{\Lambda H_s-H_s\Lambda\}=-\text{off}\{X'(\mathcal{D}_sA)X-X'(\mathcal{D}_sB)X\Lambda\}.
\end{equation} 
Switch to subscript notation and solve for $H_s$. For $s\neq t$ 
\begin{equation}
\{H_s\}_{ij}=-\frac{x_i'\{\mathcal{D}_sA-\lambda_s\mathcal{D}_sB\}x_j}{\lambda_i-\lambda_j}.\label{eq-offh}
\end{equation} 
This does not give a value for the diagonal of $H$. Differentiating \eqref{eq-gevdef2} gives 
\begin{equation}
X'B(\mathcal{D}_sX)+(\mathcal{D}_sX)'BX+X'\mathcal{D}_sBX=0.\label{eq-fromtwo}
\end{equation} 
Using $\mathcal{D}_sX=XH_s$ and taking the diagonal of \eqref{eq-fromtwo} gives 
\begin{equation}
h_{ii}=-tfrac12 x_i'(\mathcal{D}_sB)x_i. \label{eq-diagh}
\end{equation} 
Combining \eqref{eq-offh} and \eqref{eq-diagh} shows that for the eigenvector corresponding with $\lambda_s$ we have\footnote{Symbol $\cdot$ is used for multiplication with a scalar if using it makes a formula more clear, similar to parentheses.} then 
\begin{equation}
\mathcal{D}_sx_i=-\sum_{j\neq i}\frac{x_j'\{\mathcal{D}_sA-\lambda_i\mathcal{D}_sB\}x_i}{\lambda_j-\lambda_i}x_j-tfrac12 x_i'(\mathcal{D}_sB)x_i\cdot x_i.
\end{equation}
which can also be written as
\begin{equation}
\mathcal{D}_sx_i=-\sum_{j\neq i}\left\{\frac{x_jx_j'}{\lambda_j-\lambda_i}\right\}(\mathcal{D}_sA-\lambda_i\mathcal{D}_sB)x_i-tfrac12 x_i'(\mathcal{D}_sB)x_i)\cdot x_i
\end{equation}
It is convenient to have a matrix expression for this derivative. Define the matrices
\begin{equation}
W_i:=
\sum_{j\neq i}\left\{\frac{x_jx_j'}{\lambda_j-\lambda_i}\right\}=
X(\Lambda-\lambda_iI)^+X',\label{eq-geninv}
\end{equation} 
where $(\Lambda-\lambda_iI)^+$ is the Moore-Penrose inverse of $\Lambda-\lambda_iI$.
Using \eqref{eq-geninv} gives
\begin{equation}
\mathcal{D}_sx_i=-W_i(\mathcal{D}_sA-\lambda_i\mathcal{D}_sB)x_i-\tfrac12 x_i'(\mathcal{D}_sB)x_i\cdot x_i.\label{eq-singvec}
\end{equation}
Equations \eqref{eq-singval} and \eqref{eq-singvec} will be used frequently in this paper.

## Second Partials

To find second partial derivatives we start simple with the eigenvalues. Differentiate \eqref{eq-singval} with respect to $\theta_t$. This gives 
\begin{equation}
\mathcal{D}_{st}\lambda_i=2x_i'(\mathcal{D}_sA-\lambda_i\mathcal{D}_sB)\mathcal{D}_tx_i+
x_i'(D_{st}A-\lambda_s\mathcal{D}_{st}B)x_i-x_i'(\mathcal{D}_sB)x_i\cdot\mathcal{D}_t\lambda_i.\label{eq-seclbd}
\end{equation} 
Substituting from \eqref{eq-singval} and \eqref{eq-singvec} gives
\begin{multline}
\mathcal{D}_{ij}\lambda_s=-2x_s'(\mathcal{D}_iA-\lambda_s\mathcal{D}_iB)W_s (\mathcal{D}_jA-\lambda_s\mathcal{D}_jB)x_s-x_s'(\mathcal{D}_iA-\lambda_s\mathcal{D}_iB) x_s'(\mathcal{D}_jB)x_s\}\\+
x_s'(D_{ij}A-\lambda_s\mathcal{D}_{ij}B)x_s-(x_s'\mathcal{D_i}Bx_s)x_s'(\mathcal{D}_jA-\lambda_s\mathcal{D}_jB)x_s.\label{eq-seclbdmat}
\end{multline} 
Formula \eqref{eq-seclbdmat} shows that $\mathcal{D}_{ij}\lambda_s=\mathcal{D}_{ji}\lambda_s$, just as it should be.

The second partials of the generalized eigenvalues are, not surprisingly, more complicated. 
Start with 
$$
\mathcal{D}_tW_i=X(\Lambda-\lambda_iI)^+D_tX'+D_tX(\Lambda-\lambda_iI)^+X'-XVX',
$$
where for $s\neq t$
$$
v_{ij}:=\frac{\mathcal{D}_t\lambda_j-\mathcal{D}_t\lambda_i}{(\lambda_t-\lambda_i)^2},
$$
and $v_{ss}=0$. Note that $V$ is anti-symmetric.

It follows that
\begin{align}
\mathcal{D}_{st}x_i=&-\mathcal{D}_tW_i(\mathcal{D}_sA-\lambda_i\mathcal{D}_sB)x_i\notag\\
&-W_i(\mathcal{D}_{st}A-\lambda_i\mathcal{D}_{st}B)x_i\notag\\
&+\mathcal{D}_s\lambda_i\cdot W_i(\mathcal{D}_sB)x_i\notag\\
&-W_i(\mathcal{D}_{s}A-\lambda_i\mathcal{D}_{s}B)\mathcal{D}_tx_i\notag\\
&-(\mathcal{D}_tx_s)'(\mathcal{D}_iB)x_i\cdot x_i\notag\\
&-\tfrac12 x_i'(\mathcal{D}_sB)x_i\cdot\mathcal{D}_jx_s\notag\\
&-\tfrac12 x_i'(\mathcal{D}_{st}B)x_i\cdot x_s.\label{eq-hessgen}
\end{align}

We could, of course, again substitute the expressions for $\mathcal{D}_s\lambda_i$ and $\mathcal{D}_sx_i$ from \eqref{eq-singval} and \eqref{eq-singvec} into \eqref{eq-hessgen}. But computationally it makes more sense to first compute the various intermedate quantities and then use them to put the complete expression together.

# Linear Combinations

\begin{subequations}
\begin{align}
A(\theta)&=A_0+\sum_{s=1}^p\theta_sA_s,\\
B(\theta)&=B_0+\sum_{s=p+1}^{p+q}\theta_sB_s.
\end{align}
\end{subequations}

\begin{align}
\mathcal{D}_sA&=\begin{cases}A_s&\text{ if }1\leq s\leq p,\\
0&\text{ if }p+1\leq s\leq p+q,
\end{cases}\\
\mathcal{D}_sB&=\begin{cases}0&\text{ if }1\leq s\leq p,\\
B_s&\text{ if }p+1\leq s\leq p+q.
\end{cases}
\end{align}

From \eqref{eq-singval}
\begin{equation}
\mathcal{D}_s\lambda_i=\begin{cases}x_i'A_sx_i&\text{ if }1\leq s\leq p,\\
-\lambda_ix_i'B_sx_i&\text{ if }p+1\leq s\leq p+q,
\end{cases}
\end{equation}
and from \eqref{eq-singvec}
\begin{equation}
\mathcal{D}_sx_i=\begin{cases}-W_iA_sx_i&\text{ if }1\leq s\leq p,\\
\lambda_iW_iB_sx_i-tfrac12 x_i'B_sx_i\cdot x_i.&\text{ if }p+1\leq s\leq p+q,
\end{cases}
\end{equation}
The second partials simplify considerably because $\mathcal{D}_{st}A=\mathcal{D}_{st}B=0$.




## Partial Derivatives

\begin{subequations}
\begin{align}
A&=\mathop{\sum\sum}_{1\leq i\leq j}a_{ij}E_{ij},\\
B&=\mathop{\sum\sum}_{1\leq i\leq j}b_{ij}E_{ij}.
\end{align}
\end{subequations} 
Here $E_{ij}:=(e_ie_j'+e_je_i')$ for $i\neq j$ and $E_{ii}=e_ie_i'$ with $e_i$ and $e_j$ unit vectors\footnote{A unit vector $e_i$ has zeroes everywhere, except for element $i$, which is one.}. Thus \begin{equation}
\mathcal{D}_{(i,j)}A=\mathcal{D}_{(i,j)}B=E_{ij}.
\end{equation} 
and, with some improvised but obvious notation, 
\begin{subequations}
\begin{align}
\mathcal{D}_{ij}^A\lambda_s&=
\begin{cases}
2x_{is}x_{js}&\text{ if }i\neq j,\\
x_{is}^2&\text{ if }i = j.
\end{cases}\\
\mathcal{D}_{ij}^B\lambda_s&=
\begin{cases}
-2\lambda_sx_{is}x_{js}&\text{ if }i\neq j\\,
-\lambda_sx_{is}^2&\text{ if }i = j.
\end{cases}
\end{align}
\end{subequations}

\sectionbreak

# Simple Eigenvalue Decomposition

A simple eigenvalue (SEV) problem is of the form 
\begin{subequations}
\begin{align}
AX&=X\Lambda,\\
X'X&=I.
\end{align} 
\end{subequations}
Thus both $B=I$ and $\mathcal{D}B=0$. In SEV the matrix $W_i$ from \eqref{geninv} is actually the
Moore-Penrose inverse of $A-\lambda_iI$.

From \eqref{eq-singval} 
\begin{equation}
\mathcal{D}_s\lambda_i=x_i'(\mathcal{D}_sA)x_i.\label{eq-sevalsim}
\end{equation} 
and from 
\eqref{eq-singvec} 
\begin{equation}
\mathcal{D}_sx_i=-(A-\lambda_iI)^+(\mathcal{D}_sA)x_i.\label{eq-vecpersimmp}
\end{equation}


\sectionbreak

# Generalized Singular Value Decomposition

$X=A^{-1}GY\Lambda^{-1}$ then $G'E^{-1}GY=BY\Lambda^2$

$$
(\mathcal{D}G'E^{-1}G)=(\mathcal{D}G)'E^{-1}G+G'E^{-1}(\mathcal{D}G)-G'E^{-1}(\mathcal{D}E)E^{-1}G
$$ $$
\mathcal{D}\Lambda=tfrac12\text{diag}\{\Lambda^{-1}Y'(\mathcal{D}G'E^{-1}G)Y-\Lambda^{-1}Y'(\mathcal{D}B)Y\Lambda^2\}
$$ $$
\mathcal{D}\lambda_s=tfrac12 y_s'\{\lambda_s^{-1}(\mathcal{D}G'E^{-1}G)-\lambda_s(\mathcal{D}B)\}y_s
$$ \sectionbreak

# References
