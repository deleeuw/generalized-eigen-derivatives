% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  12pt,
  letterpaper,
  DIV=11,
  numbers=noendperiod]{scrartcl}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else  
    % xetex/luatex font selection
    \setmainfont[]{Times New Roman}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{5}
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother


\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother
% definitions for citeproc citations
\NewDocumentCommand\citeproctext{}{}
\NewDocumentCommand\citeproc{mm}{%
  \begingroup\def\citeproctext{#2}\cite{#1}\endgroup}
\makeatletter
 % allow citations to break across lines
 \let\@cite@ofmt\@firstofone
 % avoid brackets around text for \cite:
 \def\@biblabel#1{}
 \def\@cite#1#2{{#1\if@tempswa , #2\fi}}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-indent, #2 entry-spacing
 {\begin{list}{}{%
  \setlength{\itemindent}{0pt}
  \setlength{\leftmargin}{0pt}
  \setlength{\parsep}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
   \setlength{\leftmargin}{\cslhangindent}
   \setlength{\itemindent}{-1\cslhangindent}
  \fi
  % set entry spacing
  \setlength{\itemsep}{#2\baselineskip}}}
 {\end{list}}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{\hfill\break\parbox[t]{\linewidth}{\strut\ignorespaces#1\strut}}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

\usepackage{tcolorbox}
\usepackage{amssymb}
\usepackage{yfonts}
\usepackage{bm}


\newtcolorbox{greybox}{
  colback=white,
  colframe=blue,
  coltext=black,
  boxsep=5pt,
  arc=4pt}
  
\newcommand{\sectionbreak}{\clearpage}

 
\newcommand{\ds}[4]{\sum_{{#1}=1}^{#3}\sum_{{#2}=1}^{#4}}
\newcommand{\us}[3]{\mathop{\sum\sum}_{1\leq{#2}<{#1}\leq{#3}}}

\newcommand{\ol}[1]{\overline{#1}}
\newcommand{\ul}[1]{\underline{#1}}

\newcommand{\amin}[1]{\mathop{\text{argmin}}_{#1}}
\newcommand{\amax}[1]{\mathop{\text{argmax}}_{#1}}

\newcommand{\ci}{\perp\!\!\!\perp}

\newcommand{\mc}[1]{\mathcal{#1}}
\newcommand{\mb}[1]{\mathbb{#1}}
\newcommand{\mf}[1]{\mathfrak{#1}}

\newcommand{\eps}{\epsilon}
\newcommand{\lbd}{\lambda}
\newcommand{\alp}{\alpha}
\newcommand{\df}{=:}
\newcommand{\am}[1]{\mathop{\text{argmin}}_{#1}}
\newcommand{\ls}[2]{\mathop{\sum\sum}_{#1}^{#2}}
\newcommand{\ijs}{\mathop{\sum\sum}_{1\leq i<j\leq n}}
\newcommand{\jis}{\mathop{\sum\sum}_{1\leq j<i\leq n}}
\newcommand{\sij}{\sum_{i=1}^n\sum_{j=1}^n}
	
\KOMAoption{captions}{tableheading}
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother

\usepackage{bookmark}

\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Differentiating Generalized Eigen and Singular Value Decompositions},
  pdfauthor={Jan de Leeuw},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}


\title{Differentiating Generalized Eigen and Singular Value
Decompositions}
\author{Jan de Leeuw}
\date{February 20, 2025}

\begin{document}
\maketitle
\begin{abstract}
The derivatives of eigenvalues and eigenvectors (and singular values and
vectors) are used in many places in multivariate data analysis. This
paper reviews formulas for these derivatives and discusses several
applications. R code implementing the basic formulas and applications is
included. The results extend, generalize, correct, and improve the
results of De Leeuw (\citeproc{ref-deleeuw_R_07c}{2007}).
\end{abstract}

\renewcommand*\contentsname{Table of contents}
{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{3}
\tableofcontents
}

\sectionbreak

\textbf{Note:} This is a working manuscript which will be updated
frequently. All suggestions for improvement are welcome. All qmd, tex,
html, pdf, R, and C files are in the public domain. Attribution will be
appreciated, but is not required. Files can be found at
https://github.com/deleeuw/generalized-eigen-derivatives

\sectionbreak

\section{Introduction}\label{sec-intro}

\subsection{GEV Systems}\label{gev-systems}

Suppose \(A\) and \(B\) are real symmetric matrices of order \(n\), with
\(B\) positive definite. Generalized eigenvalues and eigenvectors are
defined as the solutions \((x,\lambda)\) of the system of equations
\begin{subequations}
\begin{align}
Ax&=\lambda Bx,\label{eq-evd1}\\
x'Bx&=1.\label{eq-evd2}
\end{align}
\end{subequations} We call this a GEV system, short for Generalized
EigenValue system.

The properties of the solutions of the system
\eqref{eq-evd1},\eqref{eq-evd2} can be found in any textbook on matrix
algebra, for example in Wilkinson (\citeproc{ref-wilkinson_65}{1965}).
We briefly summarise them here.

Solving equation \eqref{eq-evd1} is equivalent to solving the
determinantal equation \(\text{det}(A-\lambda B)=0\) for \(\lambda\). If
\(\lambda\) is a solution then \(A-\lambda B\) is singular and has a
non-trivial null-space. Any vector \(x\) in that null space satisfies
\eqref{eq-evd1}.

The determinant \(\text{det}(A-\lambda B)\) is a polynomial in
\(\lambda\) of degree \(n\), and consequently, by the Fundamental
Theorem of Algebra, has \(n\) roots. Because \(A\) is symmetric and
\(B\) is positive definite all \(n\) roots are real. Note that \(A\) can
be indefinite and/or singular, which means that some roots can be
negative or zero.

If \(\lambda_s\not=\lambda_t\) are two solutions of the determinantal
equation then the corresponding eigenvectors, which are defined up to a
scale factor, \(x_s\) and \(x_t\) are B-orthogonal,
i.e.~\(\smash{x_s'Bx_t=0}\). If \(\lambda_s\) is a root of multiplicity
\(p\) then there are \(p\) corresponding eigenvectors, spanning a
\(p\)-dimensional subspace of \(\mathbb{R}^n\), and these \(p\)
eigenvectors can be chosen to be B-orthogonal as well. If we use the
normalization in \eqref{eq-evd2} it follows that there exists a
non-singular matrix \(X\) and a diagonal \(\Lambda\) such that
\(X'BX=I\) and \(AX=BX\Lambda\), which implies \(X'AX=\Lambda\). If all
roots are different the solution \((X,\Lambda)\) is unique up to a
permutation of the columns of \(X\), and we can eliminate this
non-uniqueness by requiring that
\(\lambda_1>\lambda_2>\cdots>\lambda_n\). If there are multiple roots,
and we require \(\lambda_1\geq\lambda_2\geq\cdots\geq\lambda_n\), the
solution is unique up to a rotation within each of the subspaces
associated with multiple roots.

If \(\lambda_s\) is a simple eigenvalue, i.e.~it is different from all
other roots, then both \(\lambda_s\) and \(x_s\) are differentiable at
\((A,B)\) (see Wilkinson (\citeproc{ref-wilkinson_65}{1965}), chapter 2,
and for much more detail Kato (\citeproc{ref-kato_76}{1976})). Suppose
the matrices \(\Delta_A\) and \(\Delta_B\) are real and symmetric
perturbations.
Define\footnote{Here $o(\epsilon)$ is any function of $\epsilon$ satisfying 
$\lim_{\epsilon\rightarrow 0}o(\epsilon)/\epsilon=0$.}\footnote{The symbol $:=$ is used for definitions.}
\begin{subequations}
\begin{align}
A(\epsilon)&:=A+\epsilon\Delta_A+o(\epsilon),\\
B(\epsilon)&:=B+\epsilon\Delta_B+o(\epsilon).
\end{align}
\end{subequations} Differentiability of \(\lambda_s\) and \(x_s\)
implies that the differentials \begin{subequations}
\begin{equation}
d\lambda_s:=\lim_{\epsilon\rightarrow 0}\frac{\lambda_s(A(\epsilon),B(\epsilon))-\lambda_s(A,B)}{\epsilon},\label{eq-gder1}
\end{equation}
and 
\begin{equation}
dx_s:=\lim_{\epsilon\rightarrow 0}\frac{x_s(A(\epsilon),B(\epsilon))-x_s(A,B)}{\epsilon}\label{eq-gder2}
\end{equation}
\end{subequations} exist. Our notation surpresses the dependence of
\(d\lambda_s\) and \(dx_s\) on \((A,B)\) and on \((\Delta_A,\Delta_B)\)
because for our purposes these are just fixed constants.

\subsection{GSV Systems}\label{gsv-systems}

Suppose \(F\) is an \(n\times m\) matrix, \(G\) is a positive definite
matrix of order \(n\), and \(H\) is a positive definite matrix of order
\(m\). We suppose without loss of generality that \(n\geq m\). The
generalized singular value problem for the triple \((F,G,H)\) finds
solutions to the system \begin{subequations}
\begin{align}
Fy&=\lambda Gx,\label{eq-svd1}\\
F'x&=\lambda Hy,\label{eq-svd2}\\
x'Gx+y'Hy&=1,\label{eq-svd3}
\end{align}
\end{subequations} We refer to this as a GSV system, short for
Generalized Singular Value system.

Now consider the GEV system \begin{subequations}
\begin{equation}
\begin{bmatrix}
0&F\\
F'&0
\end{bmatrix}
\begin{bmatrix}
x\\y
\end{bmatrix}
=\lambda
\begin{bmatrix}
G&0\\
0&H
\end{bmatrix}
\begin{bmatrix}
x\\y
\end{bmatrix}\label{eq-gsevd1},
\end{equation}
with normalization
\begin{equation}
x'Gx+y'Hy=1.\label{eq-gsevd2}
\end{equation}
\end{subequations} It is easy to see that \(x, y\) and \(\lambda\)
satisfy \eqref{eq-gsevd1} and \eqref{eq-gsevd2} if and only if they
satisfy \eqref{eq-svd1}-\eqref{eq-svd3}. Note that in GSV systems the
normalization constraint \eqref{eq-svd3} is often replaced by the
constraint \(x'Gx=y'Hy=1\). We now show that this does not change the
solutions, except for multiplying the singular vectors with a scale
factor \(\frac12\sqrt{2}\).

The GSV system has \(m\) solutions with \(\Lambda\geq 0\), \(Y'HY=I\),
and \(X'GX=I\). Of these \(m\) solutions there are \(r=\text{rank}(F)\)
solutions with \(\lambda_s>0\). For each of these \(m\) solutions
\((\lambda_s,x_s,y_s)\) there is a mirror solution
\((-\lambda_s,x_s,-y_s)\), and these \(2m\) solutions are also solutions
of the GEV system. Because in GEV two solutions with different
eigenvalues are \(B\)-orthogonal it follows that for a pair of mirror
solutions with non-zero eigenvalue \(x_s'Gx_s-y_s'Hy_s=0\), and thus,
using \eqref{eq-gsevd2}, \(x_s'Gx_s=y_s'Hy_s=\frac12\). In addition
there are \(n-m\) solutions with \(\lambda_s=0\) and \(Y=0\). The
\(n-m\) eigenvectors \(X_\perp\) satisfy \(F'X_\perp=0\) and
\(\smash{X_\perp'GX_\perp=I}\). Note that if we replace \eqref{eq-svd3}
by \(x'Gx=y'Hy=1\) then the \(n-m\) solutions of
\eqref{eq-gsevd1}-\eqref{eq-gsevd2} with \(y_s=0\) are no longer
solutions of \eqref{eq-svd1}-\eqref{eq-svd3}.

In summary, the solutions of the GEV and GSV systems for the ordered
eigenvalues are \begin{subequations}
\begin{equation}
\begin{bmatrix}
\Lambda&0&-\Lambda
\end{bmatrix},\label{eq-svdsol1}
\end{equation}
and for the corresponding eigenvectors
\begin{equation}
\begin{bmatrix}
X&X_\perp&\hfill X\\
Y&0&-Y
\end{bmatrix},\label{eq-svdsol2}
\end{equation}
\end{subequations} with \(X_\perp\) a G-orthonormal basis for the
null-space of \(F'\) and \(Y_\perp\) an H-orthonormal basis for the
null-space of \(F\). The three distinct parts of the solutions in
\eqref{eq-svdsol1} and \eqref{eq-svdsol2} are of orders \(m, n-m\), and
\(m\). We are only really interested in the first \(r\) solutions, for
which the singular values can be chosen to be positive.

\sectionbreak

\section{Perturbation}\label{sec-perturb}

\subsection{Perturbed GEV Systems}\label{sec-basic}

We study the effect of symmetric perturbations
\(A+\epsilon\Delta_A+o(\epsilon)\) and
\(B+\epsilon\Delta_B+o(\epsilon)\) of \(A\) and \(B\) on the eigenvalues
\(\lambda_s\) and their corresponding eigenvectors \(x_s\). Throughout
we assume that \(\lambda_s\) is a simple eigenvalue, i.e.~that
\(\lambda_t\not=\lambda_s\) if \(t\not=s\). Note that this does not mean
all eigenvalues need to be different. Also note that if \(\epsilon\) is
small enough then \(B(\epsilon)\) is still positive definite and
\(\lambda_s(A(\epsilon),B(\epsilon))\) is still a simple eigenvalue.

To find \(d\lambda_s\) and \(dx_s\) we must solve the equations
\begin{subequations}
\begin{multline}
(A+\epsilon\Delta_A+o(\epsilon))(x_s+\epsilon dx_s+o(\epsilon))=\\(B+\epsilon\Delta_B+o(\epsilon)))(x_s+\epsilon dx_s+o(\epsilon))(\lambda_s+\epsilon d\lambda_s+o(\epsilon)),\label{eq-witheps1}
\end{multline}
and
\begin{equation}
(x_s+\epsilon dx_s+o(\epsilon))'(B+\epsilon\Delta_B+o(\epsilon))(x_s+\epsilon dx_s+o(\epsilon))=1.\label{eq-witheps2}
\end{equation}
\end{subequations}

Expand \eqref{eq-witheps1} and \eqref{eq-witheps2} and only keep the
first order terms. This gives \begin{subequations}
\begin{align}
A(dx_s)+\Delta_Ax_s&=d\lambda_sBx_s+\lambda_sB(dx_s)+\lambda_s\Delta_Bx_s,\label{eq-fo1}\\
x_s'\Delta_Bx_s+2x_s'B(dx_s)&=0.\label{eq-fo2}
\end{align}
\end{subequations} Premultiply equation \eqref{eq-fo1} with \(x_s'\).
After some simplification this gives \begin{equation}
d\lambda_s=x_s'(\Delta_A-\lambda_s\Delta_B)x_s,\label{eq-def1}
\end{equation}

We next solve for \(dx_s\). Write \(dx_s=X\alpha\), where \(X\) is any
complete set of eigenvectors from \(AX=BX\Lambda\), normalized by
\(X'BX=I\). Then \eqref{eq-fo1} becomes \begin{equation}
BX(\Lambda-\lambda_sI)\alpha=d\lambda_s Bx_s-(\Delta_Ax_s-\lambda_s\Delta_B)x_s.\label{eq-alp1}
\end{equation} Premultiplying by \(X'\) gives \begin{equation}
(\Lambda-\lambda_sI)\alpha=(d\lambda_s)e_s-X'(\Delta_A-\lambda_s\Delta_B)x_s,\label{eq-alp2}
\end{equation} with \(e_s\) a unit
vector.\footnote{A unit vector $e_s$ has element $s$ equal to one and all other elements equal to zero.}

Premultiply both sides of \eqref{eq-alp2} by \(e_s'\). The result on the
left hand side is zero, and using \eqref{eq-def1} the right hand side is
also zero. Thus \eqref{eq-alp2} cannot be used to find \(\alpha_s\). For
\(t\not= s\) we obtain \begin{equation}
\alpha_t=-\frac{x_t'(\Delta_A-\lambda_s\Delta_B)x_s}{\lambda_t-\lambda_s},\label{eq-alp3}
\end{equation} and, using \eqref{eq-fo2}, \begin{equation}
\alpha_s=-\frac12x_s'\Delta_Bx_s.\label{eq-alp3}
\end{equation} Thus \begin{equation}
dx_s=-\sum_{t\not= s}^n\frac{x_t'(\Delta_A-\lambda_s\Delta_B)x_s}{\lambda_t-\lambda_s}x_t-\frac12(x_s'\Delta_Bx_s)x_s.\label{eq-def2}
\end{equation} The two equations \eqref{eq-def1} and \eqref{eq-def2} are
the basic tools we use in this paper.

Suppose that for there is an eigenvalue \(\lambda\not=\lambda_s\) with
multiplicity \(r>1\). In that case the corresponding eigenvectors are
not uniquely defined, and it may seem that \eqref{eq-def2} may give
different results for different choices of these eigenvectors. We
briefly investigate this case. Suppose \(T\) is the index set with
\(\lambda_t=\lambda\) for all \(t\in  T\). There is an \(n\times r\)
matrix \(Y\) with \(AY=\lambda BY\), \(Y'BY=I\), unique up to a
rotation. From \eqref{eq-def2} \begin{equation}
\sum_{t\in T}\frac{y_t'(\Delta_A-\lambda_s\Delta_B)x_s}{\lambda_t-\lambda_s}y_t=\frac{YY'(\Delta_A-\lambda_s\Delta_B)x_s}{\lambda-\lambda_s},\label{eq-multi}
\end{equation} and the right hand side of \label{eq-multi} is invariant
under rotations of \(Y\). Thus it does not matter how we choose the
\(r\) eigenvectors corresponding to the multiple eigenvalue, the result
of equation \eqref{eq-multi}, and thus of \eqref{eq-def2}, will always
be the same. As long as \(\lambda_s\) is simple, we will have
differentiability of both the eigenvalue \(\lambda_s\) and the
corresponding eigenvector \(x_s\).

Equations \eqref{eq-def1} and \eqref{eq-def2} simplify in some important
special cases. For example, we can perturb \(A\) but not \(B\). Thus
\(\Delta_B=0\), and \begin{subequations}
\begin{equation}
d\lambda_s=x_s'\Delta_Ax_s\label{eq-sim1},
\end{equation}
and
\begin{equation}
dx_s=-\sum_{t\not= s}^n\frac{x_t'\Delta_Ax_s}{\lambda_t-\lambda_s}x_t.\label{eq-sim2}
\end{equation}
\end{subequations} If, in addition, \(B=I\) we have perturbation
equations for an SEV or simple eigenvalue problem. The case in which we
perturb \(B\) and not \(A\) is handled in the same way. Other special
cases and simplifications will be treated next.

\subsection{Perturbed GSV Systems}\label{sec-GSV}

We now have enough information to apply our previous perturbation
results to GSV systems. The perturbations we consider have the form
\begin{subequations}
\begin{equation}
\Delta_A=\begin{bmatrix}
0&\Delta_F\\
\Delta_F'&0
\end{bmatrix},\label{eq-asvdp}
\end{equation}
and
\begin{equation}
\Delta_B=\begin{bmatrix}
\Delta_G&0\\
0&\Delta_H
\end{bmatrix},\label{eq-bsvdp}
\end{equation}
\end{subequations} so that the perturbed system is still a GSV system.

We are only interested in perturbations of the first \(m\) singular
values and vectors. For the singular values we find \begin{equation}
d\lambda_s=2x_s'\Delta_Fy_s-\lambda_s(x_s'\Delta_Gx_s+y_s'\Delta_Hy_s).\label{eq-svdlbd}
\end{equation} Now suppose \(s\leq m\) and \(\lambda_s>0\) is a simple
singular value, not equal to any of the other \(\lambda_t\). From
\eqref{eq-def2} and from \eqref{eq-asvdp}-\eqref{eq-bsvdp} we have
\begin{align}
\begin{bmatrix}dx_s\\dy_s\end{bmatrix}=
&-\sum_{\substack{t=1\\t\not= s}}^m\frac{x_t'\Delta_Fy_s+x_s'\Delta_Fy_t-\lambda_s(x_s'\Delta_Gx_t+y_s'\Delta_Hy_t)}{\lambda_t-\lambda_s}\begin{bmatrix}x_t\\y_t\end{bmatrix}\notag\\
&+\sum_{t=m+1}^{m+n}\frac{x_t'\Delta_Fy_s-\lambda_sx_s'\Delta_Gx_t}{\lambda_s}\begin{bmatrix}x_t\\0\end{bmatrix}\notag\\
&+\sum_{\substack{t=1\\t\not= s}}^m\frac{x_t'\Delta_Fy_s-x_s'\Delta_Fy_t+\lambda_s(x_s'\Delta_Gx_t-y_s'\Delta_Hy_t)}{\lambda_t+\lambda_s}\begin{bmatrix}x_t\\-y_t\end{bmatrix}\notag\\
&-\frac12(x_s'\Delta_Gx_s+y_s'\Delta_Hy_s)\begin{bmatrix}x_s\\y_s\end{bmatrix}
\end{align}

\subsection{Parametric Perturbations}\label{sec-parametric}

If \(A\) and \(B\) are differentiable functions of a vector of \(q\)
parameters \(\theta\) then \begin{subequations}
\begin{align}
A(\theta+\epsilon d\theta)&=A(\theta)+\epsilon\sum_{r=1}^q (d\theta_r)\mathcal{D}_rA+o(\epsilon),\label{eq-par1}\\
B(\theta+\epsilon d\theta)&=B(\theta)+\epsilon\sum_{r=1}^q (d\theta_r)\mathcal{D}_rB+o(\epsilon).\label{eq-par2}
\end{align}
\end{subequations} Here the \(\mathcal{D}_rA\) is a matrix with partial
derivatives of \(A\) with respect to \(\theta_r\), evaluated at
\(\theta\), and \(d\theta_r\) is element \(r\) of the perturbation
\(d\theta\). And similarly for \(B\).

We now can apply equations \eqref{eq-def1} and \eqref{eq-def2} with
\begin{subequations}
\begin{align}
\Delta_A&=\sum_{r=1}^q(d\theta_r)\mathcal{D}_rA,\label{eq-par3}\\
\Delta_B&=\sum_{r=1}^q(d\theta_r)\mathcal{D}_rB.\label{eq-par4}
\end{align}
\end{subequations} If \(A\) and \(B\) depend on two different sets of
parameters then we can use the same equations with some of the
\(\mathcal{D}_rA\) and some of the \(\mathcal{D}_rB\) equal to zero.

\subsubsection{Linear Perturbations}\label{sec-perlinear}

In an important special case \(A\) and \(B\) are linear in \(\theta\).
So \begin{subequations}
\begin{align}
A(\theta)&=\sum_{r=1}^q\theta_r A_r,\label{eq-linpar1}\\
B(\theta)&=\sum_{r=1}^q\theta_r B_r.\label{eq-linpar2}
\end{align}
\end{subequations} In that case \(\Delta_A=\mathcal{D}_rA=A_r\) and
\(\Delta_B=\mathcal{D}_rB=B_r\).

\subsubsection{Elementwise Perturbations}\label{sec-perelementwise}

In an important special case of the linear case the parameters are all
the \(n(n+1)\) elements of \(A\) and \(B\) on and above the diagonal. We
have \begin{subequations}
\begin{align}
A&=\mathop{\sum\sum}_{1\leq i<j\leq n}a_{ij}E_{ij}+\sum_{i=1}^na_{ii}E_{i},\label{eq-elemper1}\\
B&=\mathop{\sum\sum}_{1\leq i<j\leq n}b_{ij}E_{ij}+\sum_{i=1}^nb_{ii}E_{i},\label{eq-elemper2}
\end{align}
\end{subequations} with \(\smash{E_{ij}:=e_ie_j'+e_je_i'}\) and
\(\smash{E_i:=e_ie_i'}\).

\subsection{Perturbation Code}\label{sec-pertcode}

The code in Section~\ref{sec-code} has the function perturbGeigen(),
written in R (R Core Team (\citeproc{ref-r_core_team_24}{2024})), which
has arguments \(A, B, \Delta_A,\Delta_B\) and \(p\). The argument \(p\)
is a subset of \(\{1,2,\cdots,n\}\), with \(1\leq m\leq n\) elements.
The program computes \(d\lambda_s\) and \(dx_s\) for all \(s\in p\) and
returns them, respectively, in a vector with \(m\) elements and an
\(n\times m\) matrix. The computations use \eqref{eq-sim1} and
\eqref{eq-sim2}.

The code section also has the function perturbCheck(), whichb has the
same arguments as perturbGeigen(), plus the additional argument \(eps\),
the value of \(\epsilon\). The function computes generalized eigenvalues
and eigenvectors of the pair
\((A+\epsilon\Delta_A, B+\epsilon\Delta_B)\) and compares them with the
output of perturbGeigen().

We do not give numerical examples using these functions, because they
just generate a large number of values that have no meaning. Suffices it
to say that our checks indicate the perturbation functions work well and
our approximations are as close as can be expected. Readers can use the
code to try their their own examples. We will give numerical examples in
Section~\ref{sec-applications}, where they are more meaningful.

\sectionbreak

\section{Partial Derivatives}\label{sec-partial}

\subsection{Basic Partial Derivatives}\label{sec-parbasic}

The parametric perturbation results can be easily translated into the
notation for partial derivatives. Let's introduce the notation first. If
\(f\) is a function of a vector \(\theta\) then the partial derivative
with respect to \(\theta_r\) is \(\mathcal{D}_rf\), defined by
\begin{equation}
\mathcal{D}_rf(\theta):=\lim_{\epsilon\rightarrow 0}\frac{f(\theta+\epsilon e_r)-f(\theta)}{\epsilon},\label{eq-parderdef}
\end{equation} with \(e_r\) a unit vector.

To find partial derivatives we only perturb \(\theta_r\) by setting
\(d\theta=e_r\) in our perturbation equations \eqref{eq-par3} and
\eqref{eq-par4}. We then have \(\Delta_A=\mathcal{D}_rA\) and
\(\Delta_B=\mathcal{D}_rB\), and thus \begin{subequations}
\begin{equation}
\mathcal{D}_r\lambda_s=x_s'(\mathcal{D}_rA-\lambda_s\mathcal{D}_rB)x_s,\label{eq-parper1}
\end{equation}
and
\begin{equation}
\mathcal{D}_rx_s=-\sum_{t\not= s}^n\frac{x_t'(\mathcal{D}_rA-\lambda_s\mathcal{D}_rB)x_s}{\lambda_t-\lambda_s}x_t-\frac12(x_s'\mathcal{D}_rBx_s)x_s.\label{eq-parper2}
\end{equation}
\end{subequations} For linear parametric perturbations we have the same
equations for the partial derivatives with \(\mathcal{D}_rA=A_r\) and
\(\mathcal{D}_rB=B_r\).

\subsection{Elementwise Perturbations}\label{sec-parelementwise}

For elementwise perturbations there are some useful simplifications. If
we apply \eqref{eq-elemper1} and \eqref{eq-elemper2} to
\eqref{eq-parper1} and \eqref{eq-parper2} we get \begin{subequations}
\begin{equation}
\mathcal{D}_{ij}^A\lambda_s=\begin{cases}2x_{is}x_{js}&\text{ if }i\not= j,\\
x_{is}^2&\text{ if }i=j.
\end{cases}\label{eq-elempar1}
\end{equation}
\begin{equation}
\mathcal{D}_{ij}^B\lambda_s=\begin{cases}-2\lambda_sx_{is}x_{js}&\text{ if }i\not= j,\\
-\lambda_sx_{is}^2&\text{ if }i=j.
\end{cases}
\end{equation}\label{eq-elempar2}
\begin{equation}
\mathcal{D}_{ij}^Ax_s=
\begin{cases}
-\sum_{t\not= s}^n\frac{x_{is}x_{jt}+x_{js}x_{it}}{\lambda_t-\lambda_s}x_t&\text{ if }i\not= j,\\
-\sum_{t\not= s}^n\frac{x_{is}x_{it}}{\lambda_t-\lambda_s}x_t&\text{ if }i=j.
\end{cases}\label{eq-elempar3}
\end{equation}
\begin{equation}
\mathcal{D}_{ij}^Bx_s=
\begin{cases}
\lambda_s\sum_{t\not= s}^n\frac{x_{is}x_{jt}+x_{js}x_{it}}{\lambda_t-\lambda_s}x_t-x_{is}x_{js}x_s&\text{ if }i\not= j,\\
\lambda_s\sum_{t\not= s}^n\frac{x_{is}x_{it}}{\lambda_t-\lambda_s}x_t-\frac12x_{is}^2x_s&\text{ if }i=j.
\end{cases}\label{eq-elempar4}
\end{equation}
\end{subequations} In \eqref{eq-elempar1}-\eqref{eq-elempar4} we use the
somewhat ad-hoc notation \(\mathcal{D}_{ij}^A\) and
\(\mathcal{D}_{ij}^B\) for the partial derivatives with respect to
\(a_{ij}\) and \(b_{ij}\).

As an aside, instead of deriving \eqref{eq-elempar1}-\eqref{eq-elempar4}
from \eqref{eq-parper1} and \eqref{eq-parper2} we could also have used
the chain rule to derive \eqref{eq-parper1} and \eqref{eq-parper2} from
\eqref{eq-elempar1}-\eqref{eq-elempar4}. This looks like
\begin{subequations}
\begin{align}
\mathcal{D}_r\lambda_s&=\mathop{\sum\sum}_{1\leq i\leq j\leq n}\mathcal{D}_{ij}^A\lambda_s\mathcal{D}_ra_{ij}+\mathop{\sum\sum}_{1\leq i\leq j\leq n}\mathcal{D}_{ij}^B\lambda_s\mathcal{D}_rb_{ij},\label{eq-elemchain1}\\
\mathcal{D}_rx_s&=\mathop{\sum\sum}_{1\leq i\leq j\leq n}\mathcal{D}_{ij}^Ax_s\mathcal{D}_ra_{ij}+\mathop{\sum\sum}_{1\leq i\leq j\leq n}\mathcal{D}_{ij}^Bx_s\mathcal{D}_rb_{ij}.\label{eq-elemchain2}
\end{align}
\end{subequations}

\subsection{Second Order Partials}\label{sec-second}

For various purposes in data analysis, such as Newton's method or
asymptotic bias correction, we need the second derivatives of the
eigenvalues and eigenvectors.

We start by differentiating equation \eqref{eq-parper1} with respect to
\(\theta_u\), using the abbreviations
\(C_{rs}:=\mathcal{D}_rA-\lambda_s\mathcal{D}_rB\) and
\(C_{rus}:=\mathcal{D}_{ru}A-\lambda_s\mathcal{D}_{ru}B\). This gives

\begin{equation}
\mathcal{D}_{ur}\lambda_s=2(d_ux_s)'C_{rs}x_s+
x_s'C_{rus}x_s-(d_u\lambda_s)x_s'\mathcal{D}_rBx_s,
\label{eq-secparlbd}
\end{equation}

We could expand this further by substituting \(d_u\lambda_s\) and
\(d_ux_s\) from \eqref{eq-parper1} and \eqref{eq-parper2}. But in our
computations we will use \eqref{eq-secparlbd} as is, even though
\eqref{eq-secparlbd} does not show immediately that for each \(s\) the
Hessian with elements \(\mathcal{D}_{ru}\lambda_s\) is a symmetric
matrix of order \(q\). Note that in the linear case
\(\mathcal{D}_{ru}A=\mathcal{D}_{ru}B=0\), so the middle term on the
right disappears.

The logical next step is to differentiate \eqref{eq-parper2} with
respect to \(\theta_u\). We compute the second partials for one single
element \(x_{ks}\) of \(X\) at a time. For each element \(x_{ks}\) the
Hessian will be a symmetric matrix of order \(q\). From
\eqref{eq-parper2} \begin{equation}
\mathcal{D}_rx_{ks}=-\sum_{t\not= s}^n\frac{x_t'C_{rs}x_s}{\lambda_t-\lambda_s}x_{kt}-\frac12(x_s'\mathcal{D}_rBx_s)x_{ks}.\label{eq-parper3}
\end{equation}

We start by working on the first term on the right of
\eqref{eq-parper3}. Differentiating with respect to \(\theta_u\) gives
\begin{equation}
\mathcal{D}_{u}\left\{\frac{x_t'C_{rs}x_s}{\lambda_t-\lambda_s}x_{kt}\right\}=\frac{x_t'C_{rs}x_s}{\lambda_t-\lambda_s}d_ux_{kt}+
\mathcal{D}_{u}\left\{\frac{x_t'C_{rs})x_s}{\lambda_t-\lambda_s}\right\}x_{kt}.\label{eq-hess1}
\end{equation} The derivative in the second term on the right of
\eqref{eq-hess1} evaluates to \begin{equation}
\mathcal{D}_{u}\left\{\frac{x_t'C_{rs}x_s}{\lambda_t-\lambda_s}\right\}=
\frac{(\lambda_t-\lambda_s)\mathcal{D}_u\{x_t'C_{rs}x_s\}-x_t'C_{rs}x_s(d_u\lambda_t-d_u\lambda_s)}{(\lambda_t-\lambda_s)^2}.\label{eq-hess2}
\end{equation} The derivative in the first term of the numerator on the
right of \eqref{eq-hess2} is \begin{equation}
\mathcal{D}_u\{x_t'C_{rs}x_s\}=
(d_ux_t)'C_{rs}x_s+
x_t'C_{rs}d_ux_s+
x_t'C_{rus}x_s-(d_u\lambda_s)x_t'(\mathcal{D}_rB)x_s.
\label{eq-hess3}\end{equation} And finally, differentiating the last
term in \eqref{eq-parper3}, \begin{equation}
\mathcal{D}_u\{(x_s'\mathcal{D}_rBx_s)x_{ks}\}=\{2(d_ux_s)'\mathcal{D}_rBx_s+x_s'\mathcal{D}_{ru}Bx_s\}x_{ks}+(x_s'\mathcal{D}_rBx_s)d_ux_{ks}.
\label{eq-hess4}
\end{equation}

We combine the results in \eqref{eq-hess1}-\eqref{eq-hess4} into one big
equation. The resulting formula is rather ugly, but think of it as a
recipe for calculation, not as a beautiful object in its own right.

\begin{align}
\mathcal{D}_{ru}x_{ks}=&-\sum_{t\not= s}^n\left\{
\frac{x_t'C_{rs}x_s}{\lambda_t-\lambda_s}d_ux_{kt}+\right.\notag\\
&+\frac{(d_ux_t)'C_{rs}x_s+x_t'C_{rs}(d_ux_s)+
x_t'C_{rus}x_s-(d_u\lambda_s)x_t'(\mathcal{D}_rB)x_s}{\lambda_t-\lambda_s}x_{kt}+\notag\\
&\left.-\frac{x_t'C_{rs}x_s(d_u\lambda_t-d_u\lambda_s)}{(\lambda_t-\lambda_s)^2}x_{kt}\right\}+\notag\\
&-\{(d_ux_s)'D_rBx_s+x_s'\mathcal{D}_{ru}Bx_s\}x_{ks}-\frac12(x_s'\mathcal{D}_rBx_s)d_ux_{ks}.
\label{eq-horrible}
\end{align}

There are simplifications in the linear case, where the second
derivatives of \(A\) and \(B\) are zero, and we can replace
\(\mathcal{D}_rA\) and \(\mathcal{D}_rB\) in \eqref{eq-horrible} by
\(A_r\) and \(B_r\). Thus \(C_{rs}=A_r-\lambda_sB_r\). Also
\(C_{rus}=0\) and \(\mathcal{D}_{ru}B=0\). Thus equations
\eqref{eq-secparlbd} and \eqref{eq-horrible} become \begin{equation}
\mathcal{D}_{ru}\lambda_s=2(d_ux_s)'C_{rs}x_s-d_u\lambda_sx_s'B_rx_s,
\end{equation} and \begin{align}
\mathcal{D}_{ru}x_{ks}&=-\sum_{t\not= s}\left\{\frac{x_t'C_{rs}x_s}{\lambda_t-\lambda_s}d_ux_{kt}\right.\notag\\
&+\frac{(d_ux_t)'C_{rs}x_s+
x_t'C_{rs}d_ux_s-(d_u\lambda_s)x_t'B_rx_s}{\lambda_t-\lambda_s}x_{kt}\notag\\
&\left.-\frac{x_t'C_{rs}x_s(d_u\lambda_t-d_u\lambda_s)}{(\lambda_t-\lambda_s)^2}x_{kt}\right\}\notag\\
&-((d_ux_s)'B_rx_s)x_{ks}-\frac12(x_s'B_rx_s)d_ux_{ks}.
\end{align} There are more simplifications for SEV problems where \(B\)
does not depend on \(\theta\) and thus \(\mathcal{D}B=0\).

\subsection{Partial Derivative Code}\label{sec-partialcode}

The functions partialGeigen() and partialCheck() can be used for linear
perturbations. They have both have arguments \(theta, a, b, s\), where
\(a\) and \(b\) are lists of matrices of lengths \(q\) qnd \(p\) and
\(theta\) is a vector of length \(p+q\). The last \(q\) matrices in the
list \(a\) are zero, as are the first \(p\) matrices in the list \(b\).
The index \(1\leq s\leq n\) dictates which eigen-pair we study.

partialGeigen() uses the formulas \eqref{eq-parper1} and
\eqref{eq-parper2}, while partialCheck computes numerical derivatives
using grad() and jacobian() from the numDeriv package (Gilbert and
Varadhan (\citeproc{ref-gilbert_varadhan_19}{2019})). For our example we
use the same \(A\) and \(B\) as before, and we use elementwise
perturbation. For the dominant eigenvalue partialGeigen() gives the
derivatives with respect to the elements of \(A\) and \(B\) as

We also used partialCheck() to compute numerical derivatives. The
maximum absolute difference between the numerical and analytical
partials of the eigenvalue is

The partialGeigen() function also gives the partials of the dominant
eigenvector. The partials of the three eigenvector elements with respect
to the elements of \(A\) are

and those with respect to \(B\) are

The maximum absolute differences between the numerical and analytical
partials of the eigenvector are, for \(A\) and \(B\),

In elementwise perturbation partialGeigen() uses equations
\eqref{eq-parper1} and \eqref{eq-parper2} and consequently needs lists
of binary sparse matrices as arguments. This is very wasteful, both in
memory and speed, given the fact that we also have the compact equations
\eqref{eq-elempar1} and \eqref{eq-elempar2}. We have added the more
specialized function partialElement() that uses these compact equations.
It does not use \(\theta\) and the lists with the \(A_r\) and \(B_r\).
It gives the same results as partialGeigen(), but is much faster.

The function hessianGeigenEval() computes the second partials of the
eigenvalues for linear perturbations. It has the same arguments as
partialGeigen(). The function returns a list with the second partials of
the eigenvalues.

hessianCheckEval() that computes numerical second partials. Even in our
small example the Hessian is already of order 12, so we do not show the
actual matrix. We do show that the maximum absolute difference between
the numerical and analytical second partials of the eigenvalue is

\sectionbreak

\subsection{Partial Derivatives}\label{partial-derivatives}

\section{Applications}\label{sec-applications}

\subsection{Generalized Canonical
Analysis}\label{generalized-canonical-analysis}

Suppose we have \(m\) variables, with variable \(j\) having \(k_j\)
categories or levels. This defines \(q:=\smash{\prod_{j=1}^m k_j}\)
profiles, which are binary vectors of length
\(\smash{\sum_{j=1}^m k_j}\). The data are the relative frequencies of
the profiles (cf. Gifi (\citeproc{ref-gifi_B_90}{1990}), chapter 2). If
the number of variables is large the number of profiles will be very
large, even if the number of categories for each variable is moderate.
The profile frequencies will tend to be small and many of them will be
zero.

In \begin{subequations}
\begin{align}
A&=\sum_{r=1}^qp_rg_rg_r',\\
B&=\sum_{r=1}^qp_rB_r,
\end{align}
\end{subequations} where \(p_r\) is the relative frequency of profile
\(r\), and \(g_r\) is the profile vector. Matrix \(B_r\) is diagonal,
with \(g_r\) on the diagonal.

This is a linear parametric model, and consequently we can apply the
formulas from Section~\ref{sec-perlinear} to find the derivatives of the
eigenvalues and eigenvectors with respect to the \(p_r\).

First \begin{subequations}
\begin{equation}
\mathcal{D}_r\lambda_s=x_s'(g_rg_r'-\lambda_sB_r)x_s,\label{eq-linper1}
\end{equation}
and
\begin{equation}
\mathcal{D}_rx_s=-\sum_{t\not= s}\frac{x_t'(g_rg_r'-\lambda_sD_r)x_s}{\lambda_t-\lambda_s}x_t-\frac12(x_s'D_rx_s)x_s.\label{eq-linper2}
\end{equation}
\end{subequations}

We now apply the delta method (Mann and Wald
(\citeproc{ref-mann_wald_43}{1943})), assuming that \(p\) is a
realization of a multinomial random variable \(\underline{p}\)
\footnote{We use the "Dutch convention" 
(Hemelrijk, 1966) of underlining random variables} with parameters
\((N,\pi)\) from a multinomial with probabilities \(\pi_r\). It follows
that the eigenvalues \(N^{-\frac12}(\underline{\lambda}-\lambda)\)
converges in distribution to a multivariate normal with mean zero and
covariances \[
\text{ACOV}(\underline{\lambda}_s, \underline{\lambda}_t)=\sum_{r=1}^R\pi_rx_s'(g_rg_r'-\lambda_sB_r)x_sx_t'(g_rg_r'-\lambda_tB_r)x_t,
\] assuming that \(\lambda_s\) and \(\lambda_t\) are simple eigenvalues.

\begin{equation}
\mathcal{D}_{ur}\lambda_s=2(d_ux_s)'C_{rs}x_s+
x_s'C_{rus}x_s-(d_u\lambda_s)x_s'\mathcal{D}_rBx_s,
\label{eq-hosanna}
\end{equation}

\subsection{Principal Component Analysis}\label{sec-pca}

If we think of PCA as a function of the elements of covariance matrix
then using our results is real simple. The elementwise perturbation
results and partial derivative results from
Section~\ref{sec-parelementwise} apply directly, using \(B=I\) and
\(\Delta_B=0\). If we interpret PCA as a function of the correlation
matrix matters become slightly more complicated. \(A\) is still the
covariance matrix, but \(B\) is now the diagonal of the covariance
matrix. The \(\frac12n(n+1)\) covariances are still the parameters, but
the diagonal elements contribute to both \(A\) and \(B\). Thus there are
\(\frac12n(n+1)\) design matrices \(A_r\) and \(B_r\), where the \(B_r\)
corresponding with off-diagonal elements are zero.

It is perhaps more interesting to interpret PCA as a function of the
data matrix. Suppose we have \(m\) numerical variables, with variable
\(j\) having \(k_j\) possible values. This defines
\(q:=\smash{\prod_{j=1}^m k_j}\) possible profiles, which are vectors of
length \(m\) with all combinations of the variable values. The data are
the relative frequencies of these profiles, collected in a vector \(p\)
of length \(q\). Write \(G\) for the \(q\times m\) matrix of profiles.

The GEV problem for the principal component analysis (PCA) of a
covariance matrix has \[
A=G'(P-pp')G,\\
B=I
\] while for a PCA of the correlation matrix \[
A=G'(P-pp')G,\\
B=\text{diag}(G'(P-pp')G)
\] From \ldots{}\\
\[
\mathcal{D}_rA=g_rg_r'-(\mu e_r'+e_r\mu'),
\] with \(\mu:=Gp\).

From \ldots{} \(\mathcal{D}_rB=0\). From \ldots{} \[
\mathcal{D}_rB=\text{diag}(g_rg_r')-2\mu_r
\] These can be used in \eqref{eq-parper2} and \eqref{eq-parper2}.

\[
\mathcal{D}_r\lambda_s=
x_s'\{g_rg_r'-(\mu e_r'+e_r\mu')-\lambda_s\{\text{diag}(g_rg_r')-2\mu_r\}x_s
\]

Alternative data matrix \(Z\) and PCA is an SVD of \(JZ\), which

Jackknife, Infinitesimal Jackknife

\subsection{Canonical Analysis}\label{sec-canonical}

In Canonical Analysis \[
A=\begin{bmatrix}
0&F'G\\
G'F&0
\end{bmatrix},
\] and \[
B=\begin{bmatrix}
F'F&0\\
0&G'G
\end{bmatrix}
\] Perturb \(F\) and \(G\), which gives \[
\Delta_A=\begin{bmatrix}
0&\Delta_F'G+F'\Delta_G\\
G'\Delta_F+F\Delta_G&0
\end{bmatrix}
\] and \[
\Delta_B=\begin{bmatrix}
\Delta_F'F+F'\Delta_F&0\\
0&\Delta_G'G+G'\Delta_G
\end{bmatrix}
\]

We could introduce Multiple Correspondence Analysis (MCA) as a form of
canonical analysis and use the perturbation results from
Section~\ref{sec-canonical}. Instead we go directly to a parametric
approach.

\subsection{Classical Multidimensional
Scaling}\label{classical-multidimensional-scaling}

In classical multidimensional scaling (MDS) we have a symmetric matrix
\(D\) of squared dissimilarities.

\[
\xi_{ij}=-\frac12\left\{\theta_{ij}^2-\frac{1}{n}\sum_{l=1}^n\theta_{il}^2-\frac{1}{n}\sum_{l=1}^n\theta_{lj}^2
+\frac{1}{n^2}\sum_{k=1}^n\sum_{l=1}^n\theta_{kl}^2\right\}
\] \[
A=\mathop{\sum\sum}_{1\leq i<j\leq n}\xi_{ij}E_{ij}+\sum_{i=1}^n\xi_{ii}E_i
\]

\subsection{Factor Analysis}\label{sec-fa}

\subsection{Low rank Matrix Approximation}\label{sec-rank}

\section{Discussion}\label{sec-discussion}

If \(B\) is a singular, if \(B\) is indefinite. If \(A\) and \(B\) are
not symmetric.

\section{Code}\label{sec-code}

\section*{References}\label{sec-references}
\addcontentsline{toc}{section}{References}

\phantomsection\label{refs}
\begin{CSLReferences}{1}{0}
\bibitem[\citeproctext]{ref-deleeuw_R_07c}
De Leeuw, J. 2007. {``Derivatives of Generalized Eigen Systems with
Applications.''} Preprint Series 528. Los Angeles, CA: UCLA Department
of Statistics.
\url{https://jansweb.netlify.app/publication/deleeuw-r-07-c/deleeuw-r-07-c.pdf}.

\bibitem[\citeproctext]{ref-gifi_B_90}
Gifi, A. 1990. \emph{Nonlinear Multivariate Analysis}. New York, N.Y.:
Wiley.

\bibitem[\citeproctext]{ref-gilbert_varadhan_19}
Gilbert, P., and R. Varadhan. 2019. \emph{{numDeriv: Accurate Numerical
Derivatives}}. \url{https://CRAN.R-project.org/package=numDeriv}.

\bibitem[\citeproctext]{ref-kato_76}
Kato, T. 1976. \emph{Perturbation Theory for Linear Operators}. Second
Edition. Springer.

\bibitem[\citeproctext]{ref-mann_wald_43}
Mann, H. B., and A. Wald. 1943. {``{On Stochastic Limit and Order
Relationships}.''} \emph{Annals of Mathematical Statistics} 14: 217--26.

\bibitem[\citeproctext]{ref-r_core_team_24}
R Core Team. 2024. \emph{R: A Language and Environment for Statistical
Computing}. {Vienna, Austria}: R Foundation for Statistical Computing.
\url{https://www.R-project.org/}.

\bibitem[\citeproctext]{ref-wilkinson_65}
Wilkinson, J. H. 1965. \emph{{The Algebraic Eigenvalue Problem}}.
Clarendon Press.

\end{CSLReferences}




\end{document}
